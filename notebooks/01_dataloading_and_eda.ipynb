{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header-title",
   "metadata": {},
   "source": [
    "# NETWORK INTRUSION DETECTION SYSTEM (NIDS)\n",
    "## Exploratory Data Analysis - CIC-IDS-2017\n",
    "\n",
    "---\n",
    "\n",
    "**Autore:** Emanuele Pascale\n",
    "**Anno Accademico:** 2025/2026  \n",
    "**Data:** Febbraio 2026\n",
    "\n",
    "---\n",
    "\n",
    "### Riferimenti Scientifici\n",
    "\n",
    "> **Sharafaldin, I., Lashkari, A. H., & Ghorbani, A. A.** (2018). *Toward Generating a New Intrusion Detection Dataset and Intrusion Traffic Characterization*. 4th International Conference on Information Systems Security and Privacy (ICISSP), 108-116.\n",
    "\n",
    "**Dataset Source:** [Canadian Institute for Cybersecurity - UNB](https://www.unb.ca/cic/datasets/ids-2017.html)\n",
    "\n",
    "---\n",
    "\n",
    "### Obiettivi dell'Analisi\n",
    "\n",
    "Questa **Exploratory Data Analysis (EDA)** ha l'obiettivo di:\n",
    "\n",
    "1. **Comprendere la struttura** del dataset CIC-IDS-2017 e identificare eventuali problematiche di qualità\n",
    "2. **Analizzare la distribuzione** delle classi di attacco e valutare il class imbalance\n",
    "3. **Preparare il terreno** per la fase di feature selection (Notebook 2)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Struttura del Notebook\n",
    "\n",
    "```\n",
    "FASE 1-3: DATA LOADING & PREPROCESSING\n",
    "├─ Sezione 1: Setup ambiente e configurazione\n",
    "├─ Sezione 2: Definizione helper functions\n",
    "├─ Sezione 3: Caricamento dataset (8 file CSV)\n",
    "├─ Sezione 4: Data cleaning (NaN, Inf, duplicati, negativi)\n",
    "└─ Sezione 5: Label engineering (15 → 8 categorie)\n",
    "\n",
    "FASE 4-8: EXPLORATORY DATA ANALYSIS\n",
    "├─ Sezione 6: Analisi distribuzione classi\n",
    "├─ Sezione 7: Dataset overview e statistiche base\n",
    "├─ Sezione 8: Zero/near-zero variance detection\n",
    "├─ Sezione 9: Analisi univariata (distribuzioni)\n",
    "├─ Sezione 10: Analisi bivariata (feature-target correlation)\n",
    "├─ Sezione 11: Class separability visualization\n",
    "├─ Sezione 12: Multicollinearity detection\n",
    "└─ Sezione 13: Feature selection preparation (MI + ANOVA)\n",
    "\n",
    "FASE 9: DOMAIN-SPECIFIC ANALYSIS\n",
    "└─ Sezione 14: Analisi feature di rete (TCP flags, protocolli)\n",
    "\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-1-intro",
   "metadata": {},
   "source": [
    "## SEZIONE 1: Setup Ambiente e Configurazione\n",
    "\n",
    "### Obiettivo\n",
    "\n",
    "In questa sezione iniziale configuriamo l'**ambiente di lavoro** importando le librerie necessarie e definendo i parametri globali per l'analisi.\n",
    "\n",
    "### Note Implementative\n",
    "\n",
    "- Impostiamo `random_seed=42` per **riproducibilità** dei risultati\n",
    "- Configuriamo `warnings.filterwarnings('ignore')` per output pulito\n",
    "- Utilizziamo `os.makedirs(exist_ok=True)` per creare directory se mancanti\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "id": "setup-imports",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import gc\n",
    "import warnings\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "\n",
    "# Scikit-learn: preprocessing e feature selection\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.feature_selection import (\n",
    "    mutual_info_classif, \n",
    "    f_classif, \n",
    "    VarianceThreshold\n",
    ")\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Scipy: statistiche e correlazioni\n",
    "from scipy import stats\n",
    "from scipy.stats import spearmanr, pearsonr, mannwhitneyu\n",
    "\n",
    "# Configurazione warnings (per output pulito)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Librerie importate con successo\")\n",
    "\n",
    "\n",
    "# CONFIGURAZIONE GLOBALE\n",
    "\n",
    "# Seed per riproducibilità\n",
    "np.random.seed(42)\n",
    "\n",
    "# Stile grafici\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Parametri matplotlib\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "plt.rcParams['axes.titlesize'] = 12\n",
    "plt.rcParams['axes.labelsize'] = 11\n",
    "\n",
    "# Opzioni pandas display\n",
    "pd.set_option('display.float_format', lambda x: '%.4f' % x)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "print(\"Configurazione globale applicata\")\n",
    "\n",
    "\n",
    "\n",
    "# CONFIGURAZIONE PATH\n",
    "\n",
    "DATA_PATH = \"../data\"                      # Cartella con i file CSV del dataset\n",
    "OUTPUT_PATH = \"../output\"                  # Cartella principale per output\n",
    "IMG_PATH = os.path.join(OUTPUT_PATH, \"eda_images\")         # Grafici e visualizzazioni\n",
    "FEATURES_PATH = os.path.join(OUTPUT_PATH, \"feature_analysis\")  # Analisi feature\n",
    "REPORTS_PATH = os.path.join(OUTPUT_PATH, \"reports\")        # Report CSV/pickle\n",
    "\n",
    "# Crea directory se non esistono\n",
    "for path in [OUTPUT_PATH, IMG_PATH, FEATURES_PATH, REPORTS_PATH]:\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "# Banner iniziale\n",
    "print(\"=\" * 80)\n",
    "print(\" \" * 20 + \"NIDS - CIC-IDS-2017 EDA DEFINITIVA\")\n",
    "print(\"=\" * 80)\n",
    "print(f\" Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\" Data Path: {DATA_PATH}\")\n",
    "print(f\" Output Path: {OUTPUT_PATH}\")\n",
    "print(f\" Python: {sys.version.split()[0]} | Pandas: {pd.__version__} | NumPy: {np.__version__}\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "print(\"Path configurati e directory create\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "setup-interpretation",
   "metadata": {},
   "source": [
    "## Configurazione\n",
    "\n",
    "**Seed casuale:** Impostare `np.random.seed(42)` garantisce che operazioni stocastiche (sampling, shuffle) producano sempre gli stessi risultati, quindi fondamentale per la **riproducibilità**.\n",
    "\n",
    "**Struttura directory:**\n",
    "```\n",
    "output/\n",
    "├── eda_images/        ← Grafici PNG ad alta risoluzione (300 dpi)\n",
    "├── feature_analysis/  ← CSV con ranking feature\n",
    "└── reports/           ← Report numerici e pickle objects\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-2-intro",
   "metadata": {},
   "source": [
    "## SEZIONE 2: Helper Functions\n",
    "\n",
    "### Obiettivo\n",
    "\n",
    "Definire **funzioni di utilità** riutilizzabili per:\n",
    "- Ottimizzazione memoria (conversione dtype)\n",
    "- Monitoraggio uso RAM\n",
    "- Visualizzazioni standardizzate\n",
    "- Salvataggio report automatico\n",
    "\n",
    "### Funzioni Implementate\n",
    "\n",
    "| Funzione | Scopo | Input | Output |\n",
    "|----------|-------|-------|--------|\n",
    "| `optimize_dtypes()` | Riduce memoria 50% | DataFrame | DataFrame ottimizzato |\n",
    "| `print_memory_usage()` | Mostra RAM usata | DataFrame | Print statement |\n",
    "| `plot_distribution_comparison()` | Visualizza feature per classe | DataFrame, feature | Plot salvato |\n",
    "| `save_report()` | Salva dati su file | DataFrame/dict, filename | File CSV/PKL |\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "helpers-optimize",
   "metadata": {},
   "source": [
    "def optimize_dtypes(df):\n",
    "    \"\"\"\n",
    "    Ottimizza l'uso della memoria convertendo dtype da 64bit a 32bit/16bit.\n",
    "    \n",
    "    Parametri:\n",
    "        df (pd.DataFrame): DataFrame da ottimizzare\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame con dtype ottimizzati\n",
    "        \n",
    "    Note:\n",
    "        - float64 → float32 (riduzione 50% memoria)\n",
    "        - int64 → int32 o int16 (basato su range valori)\n",
    "        - Preserva categorie e stringhe\n",
    "    \"\"\"\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        if col_type == 'float64':\n",
    "            df[col] = df[col].astype('float32')\n",
    "        elif col_type == 'int64':\n",
    "            # Scegli int16 o int32 in base al range\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                df[col] = df[col].astype('int16')\n",
    "            else:\n",
    "                df[col] = df[col].astype('int32')\n",
    "                \n",
    "    return df\n",
    "\n",
    "print(\"optimize_dtypes() definita\")\n",
    "\n",
    "\n",
    "def print_memory_usage(df, label=\"\"):\n",
    "    \"\"\"\n",
    "    Stampa informazioni sull'uso della memoria del DataFrame.\n",
    "\n",
    "    Parametri:\n",
    "        df (pd.DataFrame): DataFrame da analizzare\n",
    "        label (str): Etichetta descrittiva per output\n",
    "\n",
    "    Output:\n",
    "        Print statement con memoria in MB e dimensioni DataFrame\n",
    "    \"\"\"\n",
    "    memory_mb = df.memory_usage(deep=True).sum() / (1024 ** 2)\n",
    "    print(f\" {label}: {memory_mb:.2f} MB \"\n",
    "          f\"({df.shape[0]:,} rows × {df.shape[1]} cols)\")\n",
    "\n",
    "print(\"print_memory_usage() definita\")\n",
    "\n",
    "\n",
    "def plot_distribution_comparison(df, feature, target='Label',\n",
    "                                   save_path=None, log_scale=False):\n",
    "    \"\"\"\n",
    "    Visualizza la distribuzione di una feature per ogni classe target.\n",
    "    Utilizzata per analisi bivariate.\n",
    "\n",
    "    Parametri:\n",
    "        df (pd.DataFrame): Dataset completo\n",
    "        feature (str): Nome della feature da visualizzare\n",
    "        target (str): Nome della colonna target (default: 'Label')\n",
    "        save_path (str): Path completo cocnesnelele per salvare figura (opzionale)\n",
    "        log_scale (bool): Applica scala logaritmica all'asse y\n",
    "\n",
    "    Output:\n",
    "        Figura con 2 subplot (violin plot + box plot)\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "    # Subplot 1: Violin plot (mostra distribuzione completa)\n",
    "    sns.violinplot(x=target, y=feature, data=df, ax=axes[0],\n",
    "                   inner='quartile', scale='width')\n",
    "    axes[0].set_title(f'Distribuzione: {feature} per Classe')\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "    axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "    if log_scale:\n",
    "        axes[0].set_yscale('log')\n",
    "\n",
    "    # Subplot 2: Box plot (focus su outliers)\n",
    "    sns.boxplot(x=target, y=feature, data=df, ax=axes[1],\n",
    "                showfliers=False)  # No outliers per leggibilità\n",
    "    axes[1].set_title(f'Box Plot: {feature} per Classe')\n",
    "    axes[1].tick_params(axis='x', rotation=45)\n",
    "    axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "    if log_scale:\n",
    "        axes[1].set_yscale('log')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"   Salvato: {os.path.basename(save_path)}\")\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "print(\"plot_distribution_comparison() definita\")\n",
    "\n",
    "import json\n",
    "\n",
    "def save_report(data, filename, report_type='csv'):\n",
    "    \"\"\"\n",
    "    Salva report in formati LEGGIBILI (CSV o JSON).\n",
    "    Gestisce automaticamente liste e dizionari.\n",
    "    \"\"\"\n",
    "    filepath = os.path.join(REPORTS_PATH, filename)\n",
    "\n",
    "    if report_type == 'csv':\n",
    "        # Se è una lista semplice, la converte in DataFrame al volo\n",
    "        if isinstance(data, list):\n",
    "            pd.DataFrame(data, columns=['Value']).to_csv(filepath, index=False)\n",
    "        elif isinstance(data, pd.DataFrame):\n",
    "            data.to_csv(filepath, index=False)\n",
    "\n",
    "    elif report_type == 'json':\n",
    "        # Helper per convertire numeri NumPy (int64, float32) in tipi Python standard per JSON\n",
    "        def convert(o):\n",
    "            if isinstance(o, (np.int64, np.int32, np.int16)): return int(o)\n",
    "            if isinstance(o, (np.float64, np.float32)): return float(o)\n",
    "            if isinstance(o, np.ndarray): return o.tolist()\n",
    "            return str(o)\n",
    "\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(data, f, indent=4, default=convert)\n",
    "\n",
    "    print(f\"Report salvato: {filename}\")\n",
    "\n",
    "print(\"save_report() definita\")\n",
    "print()\n",
    "print(\"Tutte le helper functions sono state caricate con successo\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "helpers-interpretation",
   "metadata": {},
   "source": [
    "### Interpretazione delle Helper Functions\n",
    "\n",
    "**Ottimizzazione memoria:** La funzione `optimize_dtypes()` è cruciale:\n",
    "- Permette di liberare circa il 50% di memoria utilizzata dai float64 e int64\n",
    "- Speedup operazioni grazie a minore I/O memoria\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-3-intro",
   "metadata": {},
   "source": [
    "## SEZIONE 3: Data Loading\n",
    "\n",
    "### Obiettivo\n",
    "\n",
    "Caricare in memoria gli **8 file CSV** che compongono il dataset CIC-IDS-2017 e concatenarli in un unico DataFrame.\n",
    "\n",
    "### Struttura Dataset Originale\n",
    "\n",
    "Il dataset è distribuito in 8 file corrispondenti a diverse giornate di traffico:\n",
    "\n",
    "| File | Giorno | Attacchi Presenti | Dimensione (~) |\n",
    "|------|--------|-------------------|----------------|\n",
    "| `Monday-WorkingHours.pcap_ISCX.csv` | Lunedì | Solo BENIGN | ~530k righe |\n",
    "| `Tuesday-WorkingHours.pcap_ISCX.csv` | Martedì | FTP/SSH Brute Force | ~445k righe |\n",
    "| `Wednesday-workingHours.pcap_ISCX.csv` | Mercoledì | DoS/Heartbleed | ~692k righe |\n",
    "| `Thursday-Morning-WebAttacks.pcap_ISCX.csv` | Giovedì AM | Web Attacks | ~170k righe |\n",
    "| `Thursday-Afternoon-Infilteration.pcap_ISCX.csv` | Giovedì PM | Infiltration | ~288k righe |\n",
    "| `Friday-Morning.pcap_ISCX.csv` | Venerdì AM | Bot, PortScan (parziale) | ~191k righe |\n",
    "| `Friday-Afternoon-DDos.pcap_ISCX.csv` | Venerdì PM | DDoS | ~225k righe |\n",
    "| `Friday-Afternoon-PortScan.pcap_ISCX.csv` | Venerdì PM | PortScan | ~286k righe |\n",
    "\n",
    "**Totale:** ~2.83M righe × 79 colonne\n",
    "\n",
    "### Note Tecniche\n",
    "\n",
    "1. **Encoding:** I file utilizzano encoding `cp1252` (Windows-1252), non UTF-8 standard\n",
    "2. **Nomi colonne:** Contengono spazi extra (es. `\" Flow Duration \"`) che vanno rimossi con `.str.strip()`\n",
    "3. **Memoria:** Caricamento file-per-file con ottimizzazione immediata previene RAM overflow\n",
    "4. **Garbage collection:** `gc.collect()` dopo concatenazione libera memoria temporanea\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "id": "loading-concatenate",
   "metadata": {},
   "source": [
    "# STEP 1: Ricerca dei File\n",
    "\n",
    "# Trova tutti i file .csv nella cartella DATA_PATH\n",
    "all_files = sorted(glob.glob(os.path.join(DATA_PATH, \"*.csv\")))\n",
    "\n",
    "if not all_files:\n",
    "    raise FileNotFoundError(\n",
    "        f\"Nessun file CSV trovato in {DATA_PATH}.\\n\"\n",
    "    )\n",
    "\n",
    "print(f\"Trovati {len(all_files)} file CSV:\\n\")\n",
    "for i, filepath in enumerate(all_files, 1):\n",
    "    print(f\"   {i}. {os.path.basename(filepath)}\")\n",
    "\n",
    "print(f\"\\n   Caricamento in corso...\\n\")\n",
    "\n",
    "\n",
    "# STEP 2: Caricamento File per File con Gestione Errori\n",
    "\n",
    "\n",
    "dfs = []              # Lista per accumulare DataFrame\n",
    "file_info = []        # Statistiche per report\n",
    "\n",
    "for idx, filepath in enumerate(all_files, 1):\n",
    "    basename = os.path.basename(filepath)\n",
    "    try:\n",
    "        # Carica CSV con encoding specifico CIC-IDS-2017\n",
    "        df_temp = pd.read_csv(filepath, encoding='cp1252', low_memory=False)\n",
    "\n",
    "        # Pulizia nomi colonne (rimuove spazi extra)\n",
    "        df_temp.columns = df_temp.columns.str.strip()\n",
    "\n",
    "        # Ottimizzazione memoria immediata\n",
    "        df_temp = optimize_dtypes(df_temp)\n",
    "\n",
    "        # Statistiche file\n",
    "        file_info.append({\n",
    "            'File': basename,\n",
    "            'Rows': df_temp.shape[0],\n",
    "            'Memory_MB': df_temp.memory_usage(deep=True).sum() / (1024**2)\n",
    "        })\n",
    "\n",
    "        dfs.append(df_temp)\n",
    "\n",
    "        print(f\"{basename:<55} | {df_temp.shape[0]:>9,} righe\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERRORE su {basename}: {e}\")\n",
    "\n",
    "if not dfs:\n",
    "    raise ValueError(\" Nessun dataframe caricato correttamente.\")\n",
    "\n",
    "# STEP 3: Concatenazione e Pulizia Memoria\n",
    "\n",
    "print(f\"\\nConcatenazione {len(dfs)} file in corso...\")\n",
    "\n",
    "# Concatena tutti i DataFrame con reset indici\n",
    "df_raw = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Libera memoria (lista temporanea non più necessaria)\n",
    "del dfs\n",
    "gc.collect()\n",
    "\n",
    "print(f\"\\n Dataset concatenato con successo\")\n",
    "print_memory_usage(df_raw, \"Dimensioni\")\n",
    "\n",
    "# Salva statistiche caricamento\n",
    "file_stats_df = pd.DataFrame(file_info)\n",
    "save_report(file_stats_df, '01_file_loading_stats.csv', 'csv')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "loading-interpretation",
   "metadata": {},
   "source": [
    "**Prossimo step:** Data cleaning per rimuovere anomalie (NaN, duplicati, valori invalidi).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-4-intro",
   "metadata": {},
   "source": [
    "## SEZIONE 4: Data Cleaning\n",
    "\n",
    "### Obiettivo\n",
    "\n",
    "Identificare e rimuovere **anomalie nei dati** che comprometterebbero il training dei modelli:\n",
    "1. Valori **infiniti** (risultato di divisioni per zero)\n",
    "2. Valori **mancanti** (NaN)\n",
    "3. Righe **duplicate**\n",
    "4. Valori **negativi non validi** (es. durata negativa)\n",
    "\n",
    "### Rationale Scientifico\n",
    "\n",
    "> **Sharafaldin et al. (2018)** documentano nel paper originale che il tool **CICFlowMeter** (usato per generare il dataset) produce errori di calcolo in presenza di pacchetti malformati, risultando in valori infiniti o negativi per metriche temporali.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "id": "cleaning-init",
   "metadata": {},
   "source": [
    "initial_rows = df_raw.shape[0]\n",
    "cleaning_log = {}  # Dizionario per tracciare operazioni\n",
    "\n",
    "\n",
    "# STEP 1: Gestione Valori Infiniti\n",
    "\n",
    "print(\"Step 1: Gestione valori Infiniti...\")\n",
    "\n",
    "# Sostituisco infiniti con NaN (che saranno rimossi nel prossimo step)\n",
    "df_raw.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "print(\"   Valori infiniti convertiti in NaN\")\n",
    "\n",
    "# STEP 2: Rimozione Valori Mancanti (NaN)\n",
    "\n",
    "print(\"\\nStep 2: Rimozione valori mancanti (NaN)...\")\n",
    "\n",
    "rows_before_na = df_raw.shape[0]\n",
    "df_raw.dropna(inplace=True)\n",
    "nan_dropped = rows_before_na - df_raw.shape[0]\n",
    "cleaning_log['NaN_removed'] = nan_dropped\n",
    "\n",
    "print(f\"   Rimossi {nan_dropped:,} record con NaN\")\n",
    "\n",
    "\n",
    "# STEP 3: Rimozione Duplicati\n",
    "\n",
    "print(\"\\nStep 3: Rimozione duplicati...\")\n",
    "\n",
    "rows_before_dedup = df_raw.shape[0]\n",
    "df_raw.drop_duplicates(inplace=True)\n",
    "dupes_dropped = rows_before_dedup - df_raw.shape[0]\n",
    "cleaning_log['Duplicates_removed'] = dupes_dropped\n",
    "\n",
    "print(f\"   Rimossi {dupes_dropped:,} record duplicati\")\n",
    "\n",
    "# STEP 4: SANITY CHECK\n",
    "\n",
    "print(\"\\nStep 4: Sanity Check (Valori Negativi Non Validi)...\")\n",
    "\n",
    "# Feature che NON possono essere negative (fisica/temporale impossibile)\n",
    "# NOTA: Escludiamo 'Init_Win_bytes_forward/backward' dove -1 = missing value legittimo\n",
    "cols_must_be_positive = [\n",
    "    'Flow Duration',      # Durata temporale (>= 0)\n",
    "    'Flow Bytes/s',       # Rate trasmissione (>= 0)\n",
    "    'Flow Packets/s',     # Rate pacchetti (>= 0)\n",
    "    'Flow IAT Mean',      # Inter-arrival time medio (>= 0)\n",
    "    'Fwd IAT Total',      # Tempo totale forward (>= 0)\n",
    "    'Bwd IAT Total'       # Tempo totale backward (>= 0)\n",
    "]\n",
    "\n",
    "# Verifica esistenza colonne (gestione spazi nei nomi)\n",
    "target_cols = [c for c in cols_must_be_positive if c in df_raw.columns]\n",
    "\n",
    "if not target_cols:\n",
    "    print(\"    Nessuna colonna target trovata per sanity check, skip.\")\n",
    "    cleaning_log['Negative_values_removed'] = 0\n",
    "else:\n",
    "    print(f\"   Controllo valori negativi su {len(target_cols)} colonne:\")\n",
    "\n",
    "    # Conta righe con ALMENO UNA feature negativa\n",
    "    neg_mask = (df_raw[target_cols] < 0).any(axis=1)\n",
    "    neg_dropped = neg_mask.sum()\n",
    "\n",
    "    if neg_dropped > 0:\n",
    "        # Dettaglio per colonna (diagnostico)\n",
    "        print(\"\\n    Breakdown per colonna:\")\n",
    "        for col in target_cols:\n",
    "            neg_count = (df_raw[col] < 0).sum()\n",
    "            if neg_count > 0:\n",
    "                print(f\"      • {col:<25}: {neg_count:>8,} valori negativi\")\n",
    "\n",
    "        # Rimuovi righe invalide\n",
    "        rows_before_neg = df_raw.shape[0]\n",
    "        df_raw = df_raw[~neg_mask].copy()\n",
    "\n",
    "        cleaning_log['Negative_values_removed'] = neg_dropped\n",
    "        print(f\"\\n Rimossi {neg_dropped:,} record con valori negativi invalidi (errore CICFlowMeter)\")\n",
    "    else:\n",
    "        print(\"Nessun valore negativo invalido trovato\")\n",
    "        cleaning_log['Negative_values_removed'] = 0\n",
    "\n",
    "# STEP 5: Reset Index e Report Finale\n",
    "\n",
    "# Reset index dopo rimozioni\n",
    "df_raw.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Report finale cleaning\n",
    "print()\n",
    "print(f\"   Righe Iniziali:        {initial_rows:>12,}\")\n",
    "print(f\"   - NaN/Inf rimossi:     {nan_dropped:>12,}\")\n",
    "print(f\"   - Duplicati rimossi:   {dupes_dropped:>12,}\")\n",
    "print(f\"   - Negativi rimossi:    {neg_dropped:>12,}\")\n",
    "print()\n",
    "print(f\"{'-' * 60}\")\n",
    "print(f\"   RIGHE VALIDE:       {df_raw.shape[0]:>12,} \"\n",
    "      f\"({100 * df_raw.shape[0] / initial_rows:.1f}% retained)\")\n",
    "print(f\"{'-' * 60}\\n\")\n",
    "\n",
    "print_memory_usage(df_raw, \"Dataset Pulito\")\n",
    "\n",
    "# Salva log cleaning per tracciabilità\n",
    "save_report(cleaning_log, '02_cleaning_log.json', 'json')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "section-5-intro",
   "metadata": {},
   "source": [
    "## SEZIONE 5: Label Engineering\n",
    "\n",
    "### Obiettivo\n",
    "\n",
    "Trasformare le **15 label originali** in **8 macro-categorie** semanticamente coerenti per:\n",
    "1. Ridurre complessità problema di classificazione\n",
    "2. Raggruppare attacchi con caratteristiche simili\n",
    "3. Migliorare bilanciamento classi\n",
    "\n",
    "### Tassonomia Originale\n",
    "\n",
    "Il dataset CIC-IDS-2017 contiene le seguenti label:\n",
    "\n",
    "```\n",
    "BENIGN                         ← Traffico normale\n",
    "DoS Hulk                       ← Denial of Service variant\n",
    "DoS GoldenEye                  ← Denial of Service variant\n",
    "DoS slowloris                  ← Denial of Service variant\n",
    "DoS Slowhttptest               ← Denial of Service variant\n",
    "DDoS                           ← Distributed DoS\n",
    "FTP-Patator                    ← Brute force FTP\n",
    "SSH-Patator                    ← Brute force SSH\n",
    "PortScan                       ← Port scanning\n",
    "Bot                            ← Botnet IRC\n",
    "Web Attack – Brute Force       ← Web application attack\n",
    "Web Attack – XSS               ← Cross-site scripting\n",
    "Web Attack – Sql Injection     ← SQL injection\n",
    "Infiltration                   ← Multi-step penetration\n",
    "Heartbleed                     ← SSL/TLS vulnerability exploit\n",
    "```\n",
    "\n",
    "### Note Tecniche\n",
    "\n",
    "- **Encoding UTF-8:** Alcune label hanno caratteri corrotti.\n",
    "- **LabelEncoder:** Convertiamo encoding numerico per target integer.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "id": "labeling-original",
   "metadata": {},
   "source": [
    "# STEP 1: Analisi Label Originali\n",
    "\n",
    "print(\"  FASE 3: LABEL ENGINEERING\")\n",
    "\n",
    "print()\n",
    "\n",
    "print(\" Label originali uniche nel dataset:\")\n",
    "original_labels = df_raw['Label'].unique()\n",
    "print(f\"   Totale categorie: {len(original_labels)}\\n\")\n",
    "\n",
    "for lbl in sorted(original_labels):\n",
    "    count = (df_raw['Label'] == lbl).sum()\n",
    "    print(f\"   - {lbl:<40} : {count:>10,}\")\n",
    "\n",
    "\n",
    "# STEP 2: Definizione Mapping e Applicazione\n",
    "\n",
    "print(\"\\n Applicazione schema di raggruppamento...\\n\")\n",
    "\n",
    "# Label Mapping secondo Sharafaldin et al. (2018)\n",
    "label_mapping = {\n",
    "    'BENIGN': 'Benign',\n",
    "\n",
    "    # DoS Attacks (mono-source)\n",
    "    'DoS Hulk': 'DoS',\n",
    "    'DoS GoldenEye': 'DoS',\n",
    "    'DoS slowloris': 'DoS',\n",
    "    'DoS Slowhttptest': 'DoS',\n",
    "\n",
    "    # DDoS (distributed)\n",
    "    'DDoS': 'DDoS',\n",
    "\n",
    "    # Brute Force\n",
    "    'FTP-Patator': 'BruteForce',\n",
    "    'SSH-Patator': 'BruteForce',\n",
    "\n",
    "    # Port Scan\n",
    "    'PortScan': 'PortScan',\n",
    "\n",
    "    # Bot\n",
    "    'Bot': 'Bot',\n",
    "\n",
    "    # Web Attacks (gestione varianti encoding UTF-8)\n",
    "    'Web Attack ï¿½ Brute Force': 'WebAttack',  # UTF-8 corrupted\n",
    "    'Web Attack Brute Force': 'WebAttack',      # UTF-8 correct\n",
    "    'Web Attack ï¿½ XSS': 'WebAttack',\n",
    "    'Web Attack XSS': 'WebAttack',\n",
    "    'Web Attack ï¿½ Sql Injection': 'WebAttack',\n",
    "    'Web Attack Sql Injection': 'WebAttack',\n",
    "\n",
    "    # Infiltration & Exploits\n",
    "    'Infiltration': 'Other',\n",
    "    'Heartbleed': 'Other'\n",
    "}\n",
    "\n",
    "# Applica mapping\n",
    "df_raw['Label'] = df_raw['Label'].astype(str).str.strip().replace(label_mapping)\n",
    "\n",
    "# Verifica risultato\n",
    "final_labels = sorted(df_raw['Label'].unique())\n",
    "print(f\"  Label dopo raggruppamento: {len(final_labels)} categorie\")\n",
    "print(f\"   {final_labels}\\n\")\n",
    "\n",
    "\n",
    "# STEP 3: Encoding Numerico Label\n",
    "\n",
    "print(\" Creazione encoding numerico per label...\\n\")\n",
    "\n",
    "# LabelEncoder per conversione string → int\n",
    "le = LabelEncoder()\n",
    "df_raw['Label_Encoded'] = le.fit_transform(df_raw['Label'])\n",
    "\n",
    "# Mostra mapping\n",
    "label_map_dict = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "print(\"   Mapping Label → Codice Numerico:\")\n",
    "for label, code in sorted(label_map_dict.items(), key=lambda x: x[1]):\n",
    "    print(f\"      {code} : {label}\")\n",
    "\n",
    "# Salva encoder per utilizzo in Notebook 3\n",
    "save_report(le.classes_.tolist(), '03_label_encoder_classes.json', 'json')\n",
    "\n",
    "\n",
    "print(\"\\n  Label engineering completato\")\n",
    "print(f\"   Colonna 'Label': string categorica (8 classi)\")\n",
    "print(f\"   Colonna 'Label_Encoded': integer encoding (0-7)\\n\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "labeling-interpretation",
   "metadata": {},
   "source": [
    "### Interpretazione del Label Engineering\n",
    "\n",
    "**Vantaggi del raggruppamento (15→8 classi):**\n",
    "1. **Riduzione complessità:** Problema 8-class vs 15-class riduce spazio ipotesi modelli\n",
    "2. **Miglior generalizzazione:** Attacchi simili (es. 4 varianti DoS) condividono pattern feature\n",
    "3. **Bilanciamento parziale:** Le categorie rare (Heartbleed) vengono fuse con Infiltration\n",
    "\n",
    "**Trade-off:**\n",
    "- **Persa granularità:** Non distinguiamo più tra slowloris vs Hulk DoS\n",
    "- **Mixing eterogeneo:** WebAttack include sia Brute Force web che XSS/SQL injection\n",
    "\n",
    "**Prossimo step:** Analisi distribuzione classi per quantificare imbalance.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05e4cc9-6df5-4e84-8c3e-a67709d0a522",
   "metadata": {},
   "source": [
    "## SEZIONE 5.1: TCP Appendix Filtering\n",
    "\n",
    "### Obiettivo\n",
    "\n",
    "Rimuovere **artefatti TCP appendix** dal dataset, ovvero flussi di rete con un numero troppo basso di pacchetti che rappresentano connessioni incomplete o abortite, non campioni validi per l'addestramento.\n",
    "\n",
    "### Problema Documentato\n",
    "\n",
    "> **Engelen et al. (2021)** - *\"Troubleshooting an Intrusion Detection Dataset: the CICIDS2017 Case Study\"* documentano che il dataset CIC-IDS-2017 contiene una **significativa percentuale di TCP appendix artifacts**: flussi con 1-2 pacchetti totali che corrispondono a:\n",
    "> \n",
    "> - **SYN scans** (singolo pacchetto SYN senza risposta)\n",
    "> - **Connessioni TCP aborrite** (SYN + RST/ACK incompleti)\n",
    "> - **Errori di estrazione CICFlowMeter** (timeout su connessioni non terminate)\n",
    "\n",
    "Questi flussi **non rappresentano traffico di rete completo** e introducono noise nel training, degradando le performance dei modelli ML.\n",
    "\n",
    "### Soluzione\n",
    "\n",
    "Applicare un **threshold minimo di pacchetti** per flusso valido.\n",
    "### Implementazione\n",
    "\n",
    "Il filtro opera sulla **somma di pacchetti forward e backward**:\n",
    "\n",
    "```python\n",
    "total_packets = Total_Fwd_Packets + Total_Backward_Packets\n",
    "valid_flows = (total_packets >= 3)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "b14bc51d11af3f52",
   "metadata": {},
   "source": [
    "## SEZIONE 5.1: TCP Appendix Filtering (Engelen et al. 2021)\n",
    "\n",
    "print(\" TCP Appendix Filtering (Engelen et al. 2021)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Applica filtro: mantieni flussi con almeno 3 pacchetti totali\n",
    "min_packet_threshold = 3\n",
    "\n",
    "# Calcolo usando 'Total Fwd Packets' e 'Total Backward Packets'\n",
    "total_packets = df_raw['Total Fwd Packets'] + df_raw['Total Backward Packets']\n",
    "\n",
    "flows_before = len(df_raw)\n",
    "df_raw = df_raw[total_packets >= min_packet_threshold].copy()\n",
    "flows_after = len(df_raw)\n",
    "flows_removed = flows_before - flows_after\n",
    "\n",
    "print(f\"Threshold impostato: {min_packet_threshold} pacchetti minimo per flusso\")\n",
    "print(f\"   Flussi prima filtro:  {flows_before:,}\")\n",
    "print(f\"   Flussi dopo filtro:   {flows_after:,}\")\n",
    "print(f\"   Flussi rimossi:       {flows_removed:,} ({flows_removed/flows_before*100:.1f}%)\")\n",
    "\n",
    "if flows_removed > 0:\n",
    "    print(f\" Artefatti TCP appendix rimossi\")\n",
    "else:\n",
    "    print(f\" Nessun TCP appendix rilevato\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "section-6-intro",
   "metadata": {},
   "source": [
    "## SEZIONE 6: Analisi Distribuzione Classi\n",
    "\n",
    "### Obiettivo\n",
    "\n",
    "Analizzare la **distribuzione delle 8 categorie** di attacco per:\n",
    "1. Quantificare il **class imbalance**\n",
    "2. Identificare classi **maggioritarie** e **minoritarie**\n",
    "3. Definire **strategie di handling** per il modeling\n",
    "\n",
    "### Strategie di Mitigazione\n",
    "\n",
    "Per class imbalance potremmo applicare:\n",
    "1. **Resampling:** SMOTE (Synthetic Minority Oversampling), non adatto per questo dataset\n",
    "2. **Algorithmic:** `class_weight='balanced'` in classificatori, quello che adotteremo in Notebook 3\n",
    "3. **Ensemble:** BalancedRandomForest, EasyEnsemble\n",
    "4. **Metriche:** F1-score, G-mean invece di Accuracy\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "id": "distribution-save",
   "metadata": {},
   "source": [
    "#Classe minoritaria:    Infiltration (39 samples)\n",
    "# STEP 1: Calcolo Statistiche Distribuzione\n",
    "\n",
    "print(\"FASE 4: ANALISI DISTRIBUZIONE CLASSI\")\n",
    "\n",
    "# Conta occorrenze per ogni classe\n",
    "class_counts = df_raw['Label'].value_counts()\n",
    "total_samples = len(df_raw)\n",
    "\n",
    "print(\" Distribuzione Classi:\\n\")\n",
    "class_stats = []\n",
    "for label, count in class_counts.items():\n",
    "    percentage = 100 * count / total_samples\n",
    "    class_stats.append({\n",
    "        'Label': label,\n",
    "        'Count': count,\n",
    "        'Percentage': percentage\n",
    "    })\n",
    "    print(f\"   {label:<20} : {count:>10,} ({percentage:>5.3f}%)\")\n",
    "\n",
    "\n",
    "# STEP 2: Calcolo Metriche Imbalance\n",
    "\n",
    "max_class = class_counts.max()\n",
    "min_class = class_counts.min()\n",
    "imbalance_ratio = max_class / min_class\n",
    "\n",
    "print(f\"\\n⚖️  METRICHE CLASS IMBALANCE:\")\n",
    "print(f\"   Classe maggioritaria:  {class_counts.idxmax()} ({max_class:,} samples)\")\n",
    "print(f\"   Classe minoritaria:    {class_counts.idxmin()} ({min_class:,} samples)\")\n",
    "print(f\"   Imbalance Ratio (IR):  {imbalance_ratio:.1f}:1\")\n",
    "print(f\"   Percentuale min class: {100 * min_class / total_samples:.3f}%\")\n",
    "\n",
    "\n",
    "# STEP 3: Visualizzazione Distribuzione (Linear + Log Scale)\n",
    "\n",
    "print(\"Generazione visualizzazione distribuzione...\\n\")\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Subplot 1: Scala Lineare\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, len(class_counts)))\n",
    "bars1 = ax1.bar(range(len(class_counts)), class_counts.values,\n",
    "                color=colors, edgecolor='black', linewidth=0.7)\n",
    "ax1.set_xticks(range(len(class_counts)))\n",
    "ax1.set_xticklabels(class_counts.index, rotation=45, ha='right')\n",
    "ax1.set_ylabel('Numero di Campioni')\n",
    "ax1.set_title('Distribuzione Classi (Scala Lineare)', fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Aggiungi valori sopra le barre\n",
    "for i, (bar, val) in enumerate(zip(bars1, class_counts.values)):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{val:,}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Subplot 2: Scala Logaritmica\n",
    "bars2 = ax2.bar(range(len(class_counts)), class_counts.values,\n",
    "                color=colors, edgecolor='black', linewidth=0.7)\n",
    "ax2.set_yscale('log')\n",
    "ax2.set_xticks(range(len(class_counts)))\n",
    "ax2.set_xticklabels(class_counts.index, rotation=45, ha='right')\n",
    "ax2.set_ylabel('Numero di Campioni (Log Scale)')\n",
    "ax2.set_title('Distribuzione Classi (Scala Logaritmica)', fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3, which='both', axis='y')\n",
    "\n",
    "plt.suptitle('Analisi Distribuzione Attacchi - CIC-IDS-2017',\n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(IMG_PATH, '01_class_distribution.png'),\n",
    "            dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "print(\"   Grafico salvato: 01_class_distribution.png\")\n",
    "\n",
    "\n",
    "# STEP 5: Salvataggio Statistiche\n",
    "\n",
    "class_stats_df = pd.DataFrame(class_stats)\n",
    "save_report(class_stats_df, '04_class_distribution_stats.csv', 'csv')\n",
    "\n",
    "print()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "distribution-interpretation",
   "metadata": {},
   "source": [
    "### Interpretazione della Distribuzione\n",
    "\n",
    "**Impatto sul modeling:**\n",
    "1. **Accuracy è metrica ingannevole:** Un classificatore che predice sempre `Benign` otterrebbe comunque accuracy alta\n",
    "2. **Classi rare critiche:** `Infiltration` è la minaccia più pericolosa. Con solo 45 samples, alto rischio di overfitting o mancato apprendimento.\n",
    "\n",
    "\n",
    "**Soluzione:**\n",
    "> - Cross-validation stratificata obbligatoria\n",
    "> - F1-score pesato come metrica primaria\n",
    "\n",
    "**Prossimo step:** Dataset overview e statistiche descrittive feature.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def02fe231f7dc",
   "metadata": {},
   "source": [
    "## SEZIONE 7: Dataset Overview & Basic Statistics\n",
    "\n",
    "### Obiettivo\n",
    "\n",
    "Fornire una **panoramica completa** del dataset pulito:\n",
    "1. Identificare tipologie di feature (numeriche vs categoriche)\n",
    "2. Calcolare statistiche descrittive (media, mediana, std, min/max)\n",
    "3. Verificare integrità finale (no missing, no duplicates)\n",
    "\n",
    "### Feature CIC-IDS-2017\n",
    "\n",
    "Il dataset contiene **79 feature** estratte da flussi di rete usando **CICFlowMeter**:\n",
    "\n",
    "**Categorie feature:**\n",
    "- **Flow-based (8):** Duration, Packet count, Byte count, Rate\n",
    "- **Temporal (12):** IAT (Inter-Arrival Time) statistics, Active/Idle times\n",
    "- **Packet size (14):** Min/Max/Mean/Std per direzione (Forward/Backward)\n",
    "- **TCP Flags (9):** PSH, URG, FIN, SYN, ACK, RST, ECE, CWR counts\n",
    "- **Protocol-specific (6):** Subflow metrics, Bulk statistics\n",
    "- **Advanced Packet Metrics (30):** Header length, segment size, down/up ratio, etc.\n",
    "\n",
    "### Statistiche Descrittive Chiave\n",
    "\n",
    "| Statistica | Significato           | Utilità per IDS |\n",
    "|------------|-----------------------|----------------|\n",
    "| **Mean** | Valore medio          | Baseline traffico normale |\n",
    "| **Std** | Variabilità           | Alta std → possibile anomalia |\n",
    "| **Skewness** | Asimmetria            | > 2 → distribuzione code pesanti |\n",
    "| **Kurtosis** | Pesantezza delle code | > 3 → outliers significativi |\n",
    "| **Zeros %** | Percentuale zeri      | Alta % → possibile low-variance |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "id": "a9960bbf4d43856d",
   "metadata": {},
   "source": [
    "# STEP 1: Identificazione Tipologie Feature\n",
    "print(\"FASE 5: DATASET OVERVIEW\")\n",
    "print()\n",
    "\n",
    "# Identifica colonne numeriche\n",
    "numeric_features = df_raw.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Rimuovi label encoded dalla lista feature (non è una feature vera)\n",
    "if 'Label_Encoded' in numeric_features:\n",
    "    numeric_features.remove('Label_Encoded')\n",
    "\n",
    "# STEP 2: Statistiche Descrittive Base\n",
    "print(\"Statistiche Descrittive (sample prime 5 features):\\n\")\n",
    "\n",
    "# Mostra sample di 5 feature per leggibilità\n",
    "desc_stats = df_raw[numeric_features[:5]].describe()\n",
    "print(desc_stats)\n",
    "print()\n",
    "\n",
    "# Calcola statistiche complete su tutte le feature\n",
    "print(\"Calcolo statistiche complete su tutte le 78 feature...\")\n",
    "full_desc = df_raw[numeric_features].describe().T\n",
    "\n",
    "# Salva in CSV per consultazione dettagliata\n",
    "full_desc.to_csv(os.path.join(REPORTS_PATH, '05_descriptive_statistics.csv'))\n",
    "print(f\"   Statistiche complete salvate: 05_descriptive_statistics.csv\")\n",
    "print(f\"      (contiene mean, std, min, 25%, 50%, 75%, max per ogni feature)\")\n",
    "print()\n",
    "\n",
    "\n",
    "# STEP 3: Integrity Check Finale\n",
    "\n",
    "print(\" Final Integrity Check:\")\n",
    "print()\n",
    "\n",
    "# Verifica missing values\n",
    "missing_total = df_raw.isnull().sum().sum()\n",
    "print(f\"   Missing values (NaN):  {missing_total}\")\n",
    "if missing_total > 0:\n",
    "    print(\"        WARNING: Trovati NaN residui!\")\n",
    "else:\n",
    "    print(\"      Nessun valore mancante\")\n",
    "\n",
    "# Verifica duplicati\n",
    "duplicates_total = df_raw.duplicated().sum()\n",
    "print(f\"   Duplicate rows:        {duplicates_total}\")\n",
    "if duplicates_total > 0:\n",
    "    print(\"        WARNING: Trovati duplicati residui!\")\n",
    "else:\n",
    "    print(\"      Nessun duplicato\")\n",
    "\n",
    "# Verifica infiniti\n",
    "inf_total = np.isinf(df_raw.select_dtypes(include=[np.number])).sum().sum()\n",
    "print(f\"   Infinite values:       {inf_total}\")\n",
    "if inf_total > 0:\n",
    "    print(\"        WARNING: Trovati infiniti residui!\")\n",
    "else:\n",
    "    print(\"      Nessun infinito\")\n",
    "\n",
    "# Memory footprint\n",
    "memory_mb = df_raw.memory_usage(deep=True).sum() / (1024 ** 2)\n",
    "print(f\"   Memory usage:          {memory_mb:.2f} MB\")\n",
    "print()\n",
    "\n",
    "print(\"Dataset pronto per EDA\")\n",
    "print()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "150cb576ba7eb3d0",
   "metadata": {},
   "source": [
    "**Prossimo step:** Zero/near-zero variance detection per identificare feature non informative.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e3f2bccd9778f0",
   "metadata": {},
   "source": [
    "## SEZIONE 8: Zero e Near-Zero Variance Detection\n",
    "\n",
    "### Obiettivo\n",
    "\n",
    "Identificare feature con **varianza nulla o trascurabile** che non contribuiscono alla discriminazione tra classi.\n",
    "\n",
    "### Varianza e Information Content\n",
    "\n",
    "Una feature con varianza zero (tutti valori identici) ha **zero information content** per la classificazione:\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "id": "2b609b5220970baf",
   "metadata": {},
   "source": [
    "# STEP 1: Rilevamento Zero Variance\n",
    "\n",
    "print(\"FASE 6: ZERO E NEAR-ZERO VARIANCE DETECTION\")\n",
    "print()\n",
    "\n",
    "print(\"Step 1: Rilevamento feature con varianza zero...\")\n",
    "\n",
    "zero_var_features = []\n",
    "\n",
    "for col in numeric_features:\n",
    "    # Una feature ha varianza zero se ha un solo valore unico\n",
    "    if df_raw[col].nunique() == 1:\n",
    "        zero_var_features.append(col)\n",
    "\n",
    "if zero_var_features:\n",
    "    print(f\"     Trovate {len(zero_var_features)} feature a varianza zero:\\n\")\n",
    "    for feat in zero_var_features:\n",
    "        unique_val = df_raw[feat].iloc[0]\n",
    "        print(f\"      - {feat:<50} (valore costante: {unique_val})\")\n",
    "else:\n",
    "    print(\"   Nessuna feature a varianza zero\")\n",
    "\n",
    "print()\n",
    "\n",
    "# STEP 2: Rilevamento Near-Zero Variance (<1% unique values)\n",
    "\n",
    "print(\"Step 2: Rilevamento near-zero variance (<1% valori unici)...\")\n",
    "print()\n",
    "\n",
    "# Usa VarianceThreshold di scikit-learn per approccio robusto\n",
    "# Threshold 0.01 = 1% varianza normalizzata\n",
    "selector = VarianceThreshold(threshold=0.01)\n",
    "\n",
    "# Fit su sample stratificato per performance\n",
    "sample_size = min(50000, len(df_raw))\n",
    "print(f\"   Analisi su sample stratificato di {sample_size:,} righe...\")\n",
    "df_sample = df_raw.sample(n=sample_size, random_state=42)\n",
    "\n",
    "try:\n",
    "    # Fit del selector\n",
    "    selector.fit(df_sample[numeric_features])\n",
    "\n",
    "    # Identifica feature con bassa varianza\n",
    "    low_var_mask = ~selector.get_support()\n",
    "    near_zero_var_features = [feat for feat, is_low in zip(numeric_features, low_var_mask) if is_low]\n",
    "\n",
    "    if near_zero_var_features:\n",
    "        print(f\"     Trovate {len(near_zero_var_features)} feature near-zero variance:\\n\")\n",
    "\n",
    "        # Mostra top 10 con percentuale valori unici\n",
    "        for feat in near_zero_var_features[:10]:\n",
    "            unique_pct = df_raw[feat].nunique() / len(df_raw) * 100\n",
    "            zeros_pct = (df_raw[feat] == 0).sum() / len(df_raw) * 100\n",
    "            print(f\"      - {feat:<45} : {unique_pct:.4f}% unici, {zeros_pct:.2f}% zeri\")\n",
    "\n",
    "        if len(near_zero_var_features) > 10:\n",
    "            print(f\"\\n      ... e altre {len(near_zero_var_features) - 10} feature\")\n",
    "    else:\n",
    "        print(\"   Nessuna feature near-zero variance rilevata\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"     Errore nel calcolo VarianceThreshold: {e}\")\n",
    "    near_zero_var_features = []\n",
    "\n",
    "print()\n",
    "\n",
    "# STEP 3: Summary e Salvataggio\n",
    "\n",
    "# Combina liste (rimuovi duplicati)\n",
    "all_low_var = list(set(zero_var_features + near_zero_var_features))\n",
    "\n",
    "print(f\"SUMMARY VARIANCE ANALYSIS:\")\n",
    "print(f\"   Zero variance:       {len(zero_var_features)} feature\")\n",
    "print(f\"   Near-zero variance:  {len(near_zero_var_features)} feature\")\n",
    "print(f\"   {'─' * 50}\")\n",
    "print(f\"   Totale da rimuovere: {len(all_low_var)} feature ({100 * len(all_low_var) / len(numeric_features):.1f}%)\")\n",
    "print()\n",
    "\n",
    "# Salva lista per Notebook 3 (Feature Selection)\n",
    "if all_low_var:\n",
    "    save_report(all_low_var, '06_low_variance_features.json', 'json')\n",
    "\n",
    "else:\n",
    "    print(\"Nessuna feature low-variance da rimuovere\")\n",
    "\n",
    "print()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7cfc24f74bfd0ef7",
   "metadata": {},
   "source": [
    "## SEZIONE 9: Univariate Analysis (Distribuzioni)\n",
    "\n",
    "### Obiettivo\n",
    "\n",
    "Analizzare la **distribuzione univariata** delle feature più informative per:\n",
    "1. Identificare **skewness** (asimmetria) e **kurtosis** (code pesanti)\n",
    "2. Rilevare **outliers** attraverso statistiche quartili\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "id": "c99c5381a963c1fc",
   "metadata": {},
   "source": [
    "# STEP 1: Selezione Top 20 Feature per Varianza\n",
    "\n",
    "\n",
    "print(\"FASE 7: UNIVARIATE ANALYSIS (Distribuzioni)\")\n",
    "print()\n",
    "\n",
    "print(\"Selezione top 20 feature più informative per varianza...\\n\")\n",
    "\n",
    "# Calcola varianza per ogni feature\n",
    "feature_variances = df_raw[numeric_features].var().sort_values(ascending=False)\n",
    "\n",
    "# Seleziona top 20\n",
    "top_20_features = feature_variances.head(20).index.tolist()\n",
    "\n",
    "print(\" TOP 20 FEATURES BY VARIANCE:\\n\")\n",
    "for i, feat in enumerate(top_20_features, 1):\n",
    "    var_val = feature_variances[feat]\n",
    "    print(f\"   {i:2d}. {feat:<50} (Var: {var_val:.2e})\")\n",
    "\n",
    "print()\n",
    "\n",
    "\n",
    "# STEP 2: Calcolo Statistiche Distribuzione\n",
    "print(\"Calcolo statistiche distribuzione per top 20 feature...\\n\")\n",
    "\n",
    "univariate_stats = []\n",
    "\n",
    "for feat in top_20_features:\n",
    "    data = df_raw[feat]\n",
    "\n",
    "    # Calcola tutte le statistiche\n",
    "    stats_dict = {\n",
    "        'Feature': feat,\n",
    "        'Mean': data.mean(),\n",
    "        'Median': data.median(),\n",
    "        'Std': data.std(),\n",
    "        'Min': data.min(),\n",
    "        'Max': data.max(),\n",
    "        'Skewness': data.skew(),\n",
    "        'Kurtosis': data.kurtosis(),\n",
    "        'Q1': data.quantile(0.25),\n",
    "        'Q3': data.quantile(0.75),\n",
    "        'IQR': data.quantile(0.75) - data.quantile(0.25),\n",
    "        'Zeros_%': (data == 0).sum() / len(data) * 100\n",
    "    }\n",
    "\n",
    "    univariate_stats.append(stats_dict)\n",
    "\n",
    "# Crea DataFrame\n",
    "univariate_df = pd.DataFrame(univariate_stats)\n",
    "\n",
    "# Mostra sample (prime 10)\n",
    "print(\" Statistiche Univariate (Top 10 features):\\n\")\n",
    "display_cols = ['Feature', 'Mean', 'Std', 'Skewness', 'Zeros_%']\n",
    "print(univariate_df.head(10)[display_cols].to_string(index=False))\n",
    "print()\n",
    "\n",
    "# Salva completo\n",
    "save_report(univariate_df, '07_univariate_statistics.csv', 'csv')\n",
    "\n",
    "\n",
    "# STEP 3: Visualizzazione Histogram Grid (Top 12 Features)\n",
    "print(\"Generazione histogram grid per visualizzazione distribuzioni...\\n\")\n",
    "\n",
    "fig, axes = plt.subplots(3, 4, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, feat in enumerate(top_20_features[:12]):\n",
    "    ax = axes[idx]\n",
    "\n",
    "    # Estrai dati (rimuovi inf/nan residui per sicurezza)\n",
    "    data = df_raw[feat].replace([np.inf, -np.inf], np.nan).dropna()\n",
    "\n",
    "    # Determina se applicare log transform per visualizzazione\n",
    "    data_range = data.max() - data.min()\n",
    "    if data.max() > 0 and (data.max() / (data.min() + 1e-9) > 1000):\n",
    "        # Range molto ampio → usa log scale\n",
    "        data_plot = np.log10(data + 1)  # +1 per gestire zeri\n",
    "        xlabel = f'log10({feat})'\n",
    "    else:\n",
    "        data_plot = data\n",
    "        xlabel = feat\n",
    "\n",
    "    # Plot histogram\n",
    "    ax.hist(data_plot, bins=50, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "    ax.set_xlabel(xlabel, fontsize=9)\n",
    "    ax.set_ylabel('Frequenza', fontsize=9)\n",
    "    ax.set_title(f'{feat}', fontsize=10, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "    # Aggiungi linee per mean e median\n",
    "    mean_val = data.mean()\n",
    "    median_val = data.median()\n",
    "\n",
    "    if 'log' in xlabel:\n",
    "        mean_plot = np.log10(mean_val + 1)\n",
    "        median_plot = np.log10(median_val + 1)\n",
    "    else:\n",
    "        mean_plot = mean_val\n",
    "        median_plot = median_val\n",
    "\n",
    "    ax.axvline(mean_plot, color='red', linestyle='--', linewidth=1.5,\n",
    "               label=f'Mean: {mean_val:.2f}')\n",
    "    ax.axvline(median_plot, color='green', linestyle='--', linewidth=1.5,\n",
    "               label=f'Median: {median_val:.2f}')\n",
    "    ax.legend(fontsize=7, loc='upper right')\n",
    "\n",
    "plt.suptitle('Distribuzione Univariata - Top 12 Features (by Variance)',\n",
    "             fontsize=14, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(IMG_PATH, '02_univariate_distributions.png'),\n",
    "            dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "print(\"  Histogram grid salvato: 02_univariate_distributions.png\")\n",
    "print()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f67495047d9db975",
   "metadata": {},
   "source": [
    "## SEZIONE 10: Bivariate Analysis (Feature-Target Correlation)\n",
    "\n",
    "### Obiettivo\n",
    "\n",
    "Quantificare la **relazione tra ogni feature e il target** (Benign vs Attack) per:\n",
    "1. Identificare **feature discriminative** (alta correlazione)\n",
    "2. Prioritizzare feature per feature selection\n",
    "\n",
    "### Conversione Target Binario\n",
    "\n",
    "Per calcolare correlazione, convertiamo il target multi-classe (8 categorie) in **binario**:\n",
    "- `Benign` → 0\n",
    "- `Qualsiasi attacco` → 1\n",
    "\n",
    "Questo permette di identificare feature che discriminano genericamente \"anomalie\" dal traffico normale.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "id": "1fd7c6e40ec2d59b",
   "metadata": {},
   "source": [
    "# STEP 1: Preparazione Target Binario\n",
    "\n",
    "print(\" FASE 8: BIVARIATE ANALYSIS (Feature-Target Correlation)\")\n",
    "print()\\\n",
    "\n",
    "print(\" Conversione target multi-classe → binario (Benign vs Attack)...\\n\")\n",
    "\n",
    "# Crea target binario: Benign=0, Any Attack=1\n",
    "y_binary = (df_raw['Label'] != 'Benign').astype(int)\n",
    "\n",
    "print(f\"   Distribuzione target binario:\")\n",
    "print(f\"      Benign (0): {(y_binary == 0).sum():>10,} samples ({100*(y_binary==0).sum()/len(y_binary):.2f}%)\")\n",
    "print(f\"      Attack (1): {(y_binary == 1).sum():>10,} samples ({100*(y_binary==1).sum()/len(y_binary):.2f}%)\")\n",
    "print()\n",
    "\n",
    "\n",
    "# STEP 2: Sample Stratificato per Performance\n",
    "\n",
    "print(\"Creazione sample stratificato per calcolo correlazioni...\\n\")\n",
    "\n",
    "# Sample size: 100k righe (balance tra accuracy e performance)\n",
    "sample_size = min(100000, len(df_raw))\n",
    "\n",
    "# Sample casuale (non stratificato perché target binario)\n",
    "np.random.seed(42)\n",
    "indices = np.random.choice(len(df_raw), size=sample_size, replace=False)\n",
    "df_sample = df_raw.iloc[indices]\n",
    "y_sample = y_binary.iloc[indices]\n",
    "\n",
    "print(f\"   Sample size: {len(df_sample):,} righe\")\n",
    "print(f\"   Proporzioni mantenute: Benign {100*(y_sample==0).sum()/len(y_sample):.1f}%, Attack {100*(y_sample==1).sum()/len(y_sample):.1f}%\")\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "# STEP 3: Calcolo Spearman Correlation per Ogni Feature\n",
    "print(\"Calcolo Spearman correlation (robusto a outliers e non-linearità)...\\n\")\n",
    "\n",
    "target_corr_list = []\n",
    "\n",
    "for col in numeric_features:\n",
    "    try:\n",
    "        # Calcola correlazione Spearman\n",
    "        corr, p_value = spearmanr(df_sample[col], y_sample, nan_policy='omit')\n",
    "\n",
    "        target_corr_list.append({\n",
    "            'Feature': col,\n",
    "            'Spearman_Corr': abs(corr),  # Valore assoluto per ranking\n",
    "            'Corr_Sign': np.sign(corr),  # Conserva segno per interpretazione\n",
    "            'p_value': p_value\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"     Errore su feature {col}: {e}\")\n",
    "\n",
    "# Crea DataFrame e ordina per correlazione assoluta decrescente\n",
    "corr_df = pd.DataFrame(target_corr_list).sort_values('Spearman_Corr', ascending=False)\n",
    "\n",
    "print(f\" Correlazioni calcolate per {len(corr_df)} feature\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# STEP 4: Display Top 20 Feature Correlate\n",
    "\n",
    "print(\" TOP 20 FEATURES CORRELATE CON TARGET (Attack vs Benign):\\n\")\n",
    "print(corr_df.head(20)[['Feature', 'Spearman_Corr', 'p_value']].to_string(index=False))\n",
    "print()\n",
    "\n",
    "# Salva risultati completi\n",
    "save_report(corr_df, '08_feature_target_correlation.csv', 'csv')\n",
    "\n",
    "\n",
    "# STEP 5: Visualizzazione Bar Chart Top 20\n",
    "\n",
    "print(\"Generazione bar chart per visualizzazione correlazioni...\\n\")\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "top_20_corr = corr_df.head(20)\n",
    "\n",
    "# Colori: rosso per correlazione negativa, blu per positiva\n",
    "colors = ['red' if sign < 0 else 'steelblue' for sign in top_20_corr['Corr_Sign']]\n",
    "\n",
    "plt.barh(top_20_corr['Feature'], top_20_corr['Spearman_Corr'],\n",
    "         color=colors, edgecolor='black', linewidth=0.7)\n",
    "plt.xlabel('|Spearman Correlation|', fontsize=12, fontweight='bold')\n",
    "plt.title('Top 20 Features Correlate con Classificazione Attack',\n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()  # Feature più correlata in alto\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Aggiungi legenda colori\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [\n",
    "    Patch(facecolor='steelblue', edgecolor='black', label='Correlazione Positiva'),\n",
    "    Patch(facecolor='red', edgecolor='black', label='Correlazione Negativa')\n",
    "]\n",
    "plt.legend(handles=legend_elements, loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(IMG_PATH, '03_feature_target_correlation.png'),\n",
    "            dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "print(\"   Bar chart salvato: 08_feature_target_correlation.png\")\n",
    "print()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3900f335c97c78de",
   "metadata": {},
   "source": [
    "### Interpretazione della Feature-Target Correlation\n",
    "\n",
    "**Correlazioni negative (ρ < 0, colore rosso):**\n",
    "- Indicano feature che **diminuiscono** durante attacchi\n",
    "- Es: `Init Win Bytes` basso in SYN flood (incomplete handshake)\n",
    "\n",
    "\n",
    "**Prossimo step:** Visualizzare class separability per le top 4 feature.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b621efe70412af13",
   "metadata": {},
   "source": [
    "## SEZIONE 11: Class Separability Visualization\n",
    "\n",
    "### Obiettivo\n",
    "\n",
    "Visualizzare graficamente quanto le **top feature** separano le **8 classi di attacco**, usando violin plots per mostrare l'intera distribuzione per classe.\n",
    "\n",
    "\n",
    "**Vantaggi Violin Plot :**\n",
    "- Mostra **bimodalità** (es. PortScan ha 2 picchi: scansioni lente vs veloci)\n",
    "- Evidenzia **forma distribuzione** (simmetrica vs skewed)\n",
    "- Visualizza **overlap tra classi** (se violin si sovrappongono → feature poco discriminativa)\n",
    "\n",
    "### Interpretazione Separability\n",
    "\n",
    "**Alta separability:**\n",
    "- Violin plots **non si sovrappongono**\n",
    "- Mediane delle classi **distanti**\n",
    "\n",
    "**Bassa separability:**\n",
    "- Violin plots **ampiamente sovrapposti**\n",
    "- Distribuzioni **simili** tra classi\n",
    "\n",
    "### Implicazioni per Modeling\n",
    "\n",
    "Feature con **alta separability visiva** sono ottime candidate per:\n",
    "- **Decision Trees:** Permettono split netti\n",
    "- **SVM:** Facilitano costruzione iperpiani di separazione\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "id": "2e735dcbd7130718",
   "metadata": {},
   "source": [
    "# STEP 1: Sample Stratificato per Visualizzazione\n",
    "\n",
    "print(\" FASE 9: CLASS SEPARABILITY ANALYSIS\")\n",
    "print()\n",
    "\n",
    "print(\"Preparazione sample stratificato per visualizzazione...\\n\")\n",
    "\n",
    "# Sample max 5000 righe per classe\n",
    "max_samples_per_class = 5000\n",
    "\n",
    "# Sampling stratificato robusto\n",
    "df_viz = pd.concat([\n",
    "    group.sample(n=min(len(group), max_samples_per_class), random_state=42)\n",
    "    for label, group in df_raw.groupby('Label')\n",
    "], ignore_index=True)\n",
    "\n",
    "# Sanity check\n",
    "assert 'Label' in df_viz.columns, \"ATTENZIONE: 'Label' column missing!\"\n",
    "assert len(df_viz) > 0, \"ATTENZIONE: df_viz is empty!\"\n",
    "\n",
    "print(f\"   Sample size totale: {len(df_viz):,} righe\")\n",
    "print(f\"   Distribuzione classi nel sample:\")\n",
    "for label, count in df_viz['Label'].value_counts().sort_values(ascending=False).items():\n",
    "    pct = 100 * count / len(df_viz)\n",
    "    print(f\"      {label:<20} : {count:>6,} righe ({pct:>5.2f}%)\")\n",
    "print()\n",
    "\n",
    "# STEP 2: Selezione Top 4 Features\n",
    "print(\" Selezione top 4 features da analisi correlazione...\\n\")\n",
    "\n",
    "# Prendi top 4 dalla correlation analysis precedente\n",
    "top_4_features = corr_df.head(4)['Feature'].tolist()\n",
    "\n",
    "print(\"   Feature selezionate per separability visualization:\")\n",
    "for i, feat in enumerate(top_4_features, 1):\n",
    "    corr_val = corr_df[corr_df['Feature'] == feat]['Spearman_Corr'].values[0]\n",
    "    print(f\"      {i}. {feat:<50} (|ρ| = {corr_val:.3f})\")\n",
    "print()\n",
    "\n",
    "\n",
    "# STEP 3: Generazione Grid 2x2 di Violin Plots\n",
    "print(\"Generazione violin plots per separability visualization...\\n\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, feat in enumerate(top_4_features):\n",
    "    ax = axes[idx]\n",
    "\n",
    "    # Violin plot con quartili interni\n",
    "    sns.violinplot(\n",
    "        x='Label',\n",
    "        y=feat,\n",
    "        data=df_viz,\n",
    "        ax=ax,\n",
    "        palette='Set2',      # Colori distinti per classe\n",
    "        inner='quartile',    # Mostra Q1, Q2, Q3\n",
    "        scale='width',       # Larghezza proporzionale a count\n",
    "        cut=0                # No estensione oltre min/max\n",
    "    )\n",
    "\n",
    "    ax.set_title(f'Distribuzione: {feat}', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('', fontsize=10)\n",
    "    ax.set_ylabel(feat, fontsize=10)\n",
    "    ax.tick_params(axis='x', rotation=45, labelsize=9)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "    # Applica log scale se range molto ampio\n",
    "    data_range = df_viz[feat].max() / (df_viz[feat].min() + 1e-9)\n",
    "    if data_range > 1000:\n",
    "        ax.set_yscale('log')\n",
    "        ax.set_ylabel(f'{feat} (log scale)', fontsize=10)\n",
    "\n",
    "plt.suptitle('Class Separability Analysis - Top 4 Features',\n",
    "             fontsize=14, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(IMG_PATH, '04_class_separability.png'),\n",
    "            dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "print(\"   Violin plots salvati: 04_class_separability.png\")\n",
    "print()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "72bc9753dbf17fbd",
   "metadata": {},
   "source": [
    "## SEZIONE 12: Multicollinearity Detection\n",
    "\n",
    "### Obiettivo\n",
    "\n",
    "Identificare **coppie di feature altamente correlate** (ridondanti) per:\n",
    "1. Ridurre **dimensionalità** senza perdita di informazione\n",
    "2. Migliorare **interpretabilità** modelli (evitare feature duplicate)\n",
    "3. Velocizzare **training** rimuovendo ridondanza\n",
    "\n",
    "### Teoria: Multicollinearità\n",
    "\n",
    "Due feature $X_1$ e $X_2$ sono **multicollineari** se:\n",
    "$$|Corr(X_1, X_2)| > \\theta$$\n",
    "\n",
    "dove $\\theta$ è una soglia.\n",
    "\n",
    "### Strategia di Rimozione\n",
    "\n",
    "Per ogni coppia $(X_1, X_2)$ con $|r| > 0.95$:\n",
    "1. **Calcola** correlazione con target per entrambe\n",
    "2. **Mantieni** la feature più correlata al target\n",
    "3. **Rimuovi** l'altra\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "id": "23fcaec709730ac6",
   "metadata": {},
   "source": [
    "# STEP 1: Calcolo Matrice di Correlazione (Pearson)\n",
    "print(\" FASE 10: MULTICOLLINEARITY DETECTION\")\n",
    "print()\n",
    "\n",
    "print(\"Calcolo matrice di correlazione (Pearson) tra feature...\\n\")\n",
    "\n",
    "# Sample per performance (50k righe sufficienti per correlazioni stabili)\n",
    "sample_corr = df_raw[numeric_features].sample(\n",
    "    n=min(50000, len(df_raw)),\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"   Sample size: {len(sample_corr):,} righe\")\n",
    "print(f\"   Feature: {len(numeric_features)}\")\n",
    "print(f\"   Matrice dimensione: {len(numeric_features)} × {len(numeric_features)}\")\n",
    "print()\n",
    "print(\"   Calcolo in corso (può richiedere 1-2 minuti)...\")\n",
    "\n",
    "# Calcola matrice correlazione (usa Pearson per linearità)\n",
    "corr_matrix = sample_corr.corr(method='pearson').abs()\n",
    "\n",
    "print(\"   Matrice calcolata\")\n",
    "print()\n",
    "\n",
    "\n",
    "# STEP 2: Estrazione Coppie con Alta Correlazione (|r| > 0.95)\n",
    "print(\"Ricerca coppie altamente correlate (|r| > 0.95)...\\n\")\n",
    "\n",
    "# Estrai upper triangle (evita duplicati e diagonale)\n",
    "upper_triangle = corr_matrix.where(\n",
    "    np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)\n",
    ")\n",
    "\n",
    "# Trova coppie con correlazione > 0.95\n",
    "high_corr_pairs = []\n",
    "\n",
    "for column in upper_triangle.columns:\n",
    "    # Feature correlate con 'column'\n",
    "    correlated_features = upper_triangle.index[upper_triangle[column] > 0.95].tolist()\n",
    "\n",
    "    for corr_feat in correlated_features:\n",
    "        high_corr_pairs.append({\n",
    "            'Feature_1': column,\n",
    "            'Feature_2': corr_feat,\n",
    "            'Correlation': upper_triangle.loc[corr_feat, column]\n",
    "        })\n",
    "\n",
    "if high_corr_pairs:\n",
    "    pairs_df = pd.DataFrame(high_corr_pairs).sort_values('Correlation', ascending=False)\n",
    "\n",
    "    print(f\"     Trovate {len(pairs_df)} coppie con |r| > 0.95:\\n\")\n",
    "    print(pairs_df.head(15).to_string(index=False))\n",
    "\n",
    "    if len(pairs_df) > 15:\n",
    "        print(f\"\\n   ... e altre {len(pairs_df) - 15} coppie\")\n",
    "else:\n",
    "    print(\"   Nessuna coppia con correlazione > 0.95 trovata\")\n",
    "    pairs_df = pd.DataFrame()\n",
    "\n",
    "print()\n",
    "\n",
    "\n",
    "# STEP 3: Identificazione Candidati per Rimozione\n",
    "\n",
    "if not pairs_df.empty:\n",
    "    print(\" Identificazione feature da rimuovere...\\n\")\n",
    "\n",
    "    # Strategia semplice: mantieni Feature_1, rimuovi Feature_2 in ogni coppia\n",
    "    # (Feature_1 appare prima alfabeticamente/per importanza)\n",
    "    drop_candidates = list(set(pairs_df['Feature_2'].tolist()))\n",
    "\n",
    "    print(f\"    Candidate per rimozione: {len(drop_candidates)} feature\")\n",
    "    print(f\"      (strategia: mantieni Feature_1 in ogni coppia)\\n\")\n",
    "\n",
    "    # Mostra prime 10\n",
    "    for i, feat in enumerate(drop_candidates[:10], 1):\n",
    "        print(f\"      {i:2d}. {feat}\")\n",
    "\n",
    "    if len(drop_candidates) > 10:\n",
    "        print(f\"      ... e altre {len(drop_candidates) - 10} feature\")\n",
    "\n",
    "    # Salva report\n",
    "    save_report(pairs_df, '09_multicollinearity_pairs.csv', 'csv')\n",
    "    # Salva lista drop candidates in JSON leggibile\n",
    "    if drop_candidates:\n",
    "        save_report(drop_candidates, '10_multicollinearity_drop_candidates.json', 'json')\n",
    "\n",
    "\n",
    "    print()\n",
    "else:\n",
    "    drop_candidates = []\n",
    "    print(\"Nessuna feature ridondante da rimuovere\")\n",
    "    print()\n",
    "\n",
    "\n",
    "# STEP 4: Visualizzazione Heatmap (Top 20 Features)\n",
    "\n",
    "if not pairs_df.empty:\n",
    "    print(\"Generazione heatmap per visualizzazione correlazioni...\\n\")\n",
    "\n",
    "    # Seleziona top 20 feature da feature-target correlation\n",
    "    top_20_for_heatmap = corr_df.head(20)['Feature'].tolist()\n",
    "\n",
    "    # Filtra solo feature presenti in corr_matrix\n",
    "    top_20_in_matrix = [f for f in top_20_for_heatmap if f in corr_matrix.columns]\n",
    "\n",
    "    plt.figure(figsize=(14, 12))\n",
    "    sns.heatmap(\n",
    "        corr_matrix.loc[top_20_in_matrix, top_20_in_matrix],\n",
    "        annot=False,  # No numeri (troppi per leggibilità)\n",
    "        cmap='coolwarm',  # Rosso=alta, Blu=bassa\n",
    "        center=0,\n",
    "        vmin=-1,\n",
    "        vmax=1,\n",
    "        square=True,\n",
    "        linewidths=0.5,\n",
    "        cbar_kws={'label': 'Pearson Correlation', 'shrink': 0.8}\n",
    "    )\n",
    "    plt.title('Correlation Heatmap - Top 20 Features',\n",
    "              fontsize=14, fontweight='bold')\n",
    "    plt.xticks(rotation=45, ha='right', fontsize=8)\n",
    "    plt.yticks(rotation=0, fontsize=8)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(IMG_PATH, '05_correlation_heatmap.png'),\n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    print(\"   Heatmap salvata: 05_correlation_heatmap.png\")\n",
    "    print()\n",
    "\n",
    "print(\"Multicollinearity detection completata\")\n",
    "print()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "eea9185af5de5567",
   "metadata": {},
   "source": [
    "## SEZIONE 13 (DEPRECATA e SPOSTATA NELLA SEZIONE SUCCESSIVA NOTEBOOK 02): \n",
    "Feature Selection Preparation (Information Gain & ANOVA)\n",
    "\n",
    "### Obiettivo\n",
    "\n",
    "Applicare **metodi statistici di feature selection** per creare un **consensus ranking** delle feature più informative:\n",
    "1. **Mutual Information (MI):** Misura dipendenza non-lineare feature-target\n",
    "2. **ANOVA F-statistic:** Testa differenza medie tra classi\n",
    "3. **Consensus Ranking:** Combina 3 metriche (MI + ANOVA + Spearman) per ridurre bias\n",
    "\n",
    "### Consensus Ranking: Best of Both Worlds\n",
    "\n",
    "Combinando 3 metriche diverse riduciamo **bias algoritmico**:\n",
    "- Feature ranked alta da **tutte e 3** → robustamente informativa\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "id": "297fc2c2f46a4f19",
   "metadata": {},
   "source": [
    "# STEP 1: Sample Stratificato per Computational Efficiency\n",
    "print(\" FASE 11: FEATURE SELECTION METRICS (Information Gain & ANOVA)\")\n",
    "print()\n",
    "\n",
    "print(\"Preparazione sample stratificato per calcoli computazionalmente costosi...\\n\")\n",
    "\n",
    "#  METODO ROBUSTO: Sample per gruppo mantenendo struttura\n",
    "max_samples_per_class = 5000\n",
    "\n",
    "df_fs_sample = pd.concat([\n",
    "    group.sample(n=min(len(group), max_samples_per_class), random_state=42)\n",
    "    for label, group in df_raw.groupby('Label')\n",
    "], ignore_index=True)\n",
    "\n",
    "# Sanity check critico\n",
    "assert 'Label' in df_fs_sample.columns, \" ATTENZIONE: 'Label' column missing!\"\n",
    "assert len(df_fs_sample) > 0, \" ATTENZIONE: df_fs_sample is empty!\"\n",
    "\n",
    "X_sample = df_fs_sample[numeric_features].fillna(0)  # MI non tollera NaN\n",
    "y_sample = df_fs_sample['Label']\n",
    "\n",
    "print(f\"    Sample size: {len(df_fs_sample):,} righe\")\n",
    "print(f\"   Classi: {len(y_sample.unique())}\")\n",
    "print(f\"   Feature: {len(numeric_features)}\")\n",
    "print()\n",
    "\n",
    "# Mostra distribuzione sample\n",
    "print(\"   Distribuzione classi nel sample:\")\n",
    "for label, count in y_sample.value_counts().sort_values(ascending=False).items():\n",
    "    pct = 100 * count / len(df_fs_sample)\n",
    "    print(f\"      {label:<20} : {count:>5,} righe ({pct:>5.2f}%)\")\n",
    "print()\n",
    "\n",
    "\n",
    "# STEP 2: Calcolo Mutual Information (Information Gain)\n",
    "print(\"Step 1: Calcolo Mutual Information...\\n\")\n",
    "print(\"   Questo può richiedere 2-3 minuti...\")\n",
    "\n",
    "try:\n",
    "    # Calcola MI per classificazione multi-classe\n",
    "    mi_scores = mutual_info_classif(\n",
    "        X_sample,\n",
    "        y_sample,\n",
    "        discrete_features=False,  # Feature continue\n",
    "        n_neighbors=5,            # Parametro KNN per stima densità\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Crea DataFrame risultati\n",
    "    mi_df = pd.DataFrame({\n",
    "        'Feature': numeric_features,\n",
    "        'MI_Score': mi_scores\n",
    "    }).sort_values('MI_Score', ascending=False)\n",
    "\n",
    "    print(\"\\n    Mutual Information calcolata\\n\")\n",
    "    print(\" TOP 20 FEATURES BY MUTUAL INFORMATION:\\n\")\n",
    "    print(mi_df.head(20).to_string(index=False))\n",
    "    print()\n",
    "\n",
    "    # Salva risultati\n",
    "    save_report(mi_df, '11_mutual_information_scores.csv', 'csv')\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n     Errore nel calcolo MI: {e}\")\n",
    "    print(\"   Creazione placeholder con score=0...\\n\")\n",
    "    mi_df = pd.DataFrame({\n",
    "        'Feature': numeric_features,\n",
    "        'MI_Score': 0\n",
    "    })\n",
    "\n",
    "\n",
    "# STEP 2b: Visualizzazione Mutual Information\n",
    "\n",
    "if mi_df['MI_Score'].sum() > 0:  # Solo se calcolo ha successo\n",
    "    print(\"Generazione bar chart Mutual Information...\\n\")\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    top_20_mi = mi_df.head(20)\n",
    "\n",
    "    plt.barh(top_20_mi['Feature'], top_20_mi['MI_Score'],\n",
    "             color='coral', edgecolor='black', linewidth=0.7)\n",
    "    plt.xlabel('Mutual Information Score', fontsize=12, fontweight='bold')\n",
    "    plt.title('Top 20 Features by Information Gain',\n",
    "              fontsize=14, fontweight='bold')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.grid(axis='x', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(IMG_PATH, '06_mutual_information.png'),\n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    print(\"   Bar chart salvato: 06_mutual_information.png\")\n",
    "    print()\n",
    "\n",
    "\n",
    "# STEP 3: Calcolo ANOVA F-Statistics\n",
    "\n",
    "print(\"Step 2: Calcolo ANOVA F-Statistics...\\n\")\n",
    "\n",
    "try:\n",
    "    # Calcola F-statistic e p-value per ogni feature\n",
    "    F_stats, p_values = f_classif(X_sample, y_sample)\n",
    "\n",
    "    # Crea DataFrame risultati\n",
    "    anova_df = pd.DataFrame({\n",
    "        'Feature': numeric_features,\n",
    "        'F_Statistic': F_stats,\n",
    "        'p_value': p_values\n",
    "    }).sort_values('F_Statistic', ascending=False)\n",
    "\n",
    "    print(\"   ANOVA F-Statistics calcolata\\n\")\n",
    "    print(\" TOP 20 FEATURES BY ANOVA F-STATISTIC:\\n\")\n",
    "    print(anova_df.head(20)[['Feature', 'F_Statistic', 'p_value']].to_string(index=False))\n",
    "    print()\n",
    "\n",
    "    # Salva risultati\n",
    "    save_report(anova_df, '12_anova_f_statistics.csv', 'csv')\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"     Errore nel calcolo ANOVA: {e}\")\n",
    "    print(\"   Creazione placeholder con F=0...\\n\")\n",
    "    anova_df = pd.DataFrame({\n",
    "        'Feature': numeric_features,\n",
    "        'F_Statistic': 0,\n",
    "        'p_value': 1\n",
    "    })\n",
    "\n",
    "\n",
    "# STEP 4: Creazione Consensus Ranking\n",
    "\n",
    "print(\"  Step 3: Creazione Consensus Ranking (MI + ANOVA + Correlation)...\\n\")\n",
    "\n",
    "try:\n",
    "    # Inizializza scaler per normalizzazione\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    # Crea DataFrame base\n",
    "    consensus_df = pd.DataFrame({'Feature': numeric_features})\n",
    "\n",
    "    # Merge tutti gli score\n",
    "    consensus_df = consensus_df.merge(\n",
    "        mi_df[['Feature', 'MI_Score']], on='Feature', how='left'\n",
    "    ).merge(\n",
    "        anova_df[['Feature', 'F_Statistic']], on='Feature', how='left'\n",
    "    ).merge(\n",
    "        corr_df[['Feature', 'Spearman_Corr']], on='Feature', how='left'\n",
    "    )\n",
    "\n",
    "    # Riempi eventuali NaN con 0\n",
    "    consensus_df.fillna(0, inplace=True)\n",
    "\n",
    "    # Normalizza ciascuno score a [0, 1]\n",
    "    consensus_df['MI_Norm'] = scaler.fit_transform(consensus_df[['MI_Score']])\n",
    "    consensus_df['ANOVA_Norm'] = scaler.fit_transform(consensus_df[['F_Statistic']])\n",
    "    consensus_df['Corr_Norm'] = consensus_df['Spearman_Corr']  # Già in [0,1]\n",
    "\n",
    "    # Calcola Consensus Score (media semplice dei 3 score normalizzati)\n",
    "    consensus_df['Consensus_Score'] = (\n",
    "        consensus_df['MI_Norm'] +\n",
    "        consensus_df['ANOVA_Norm'] +\n",
    "        consensus_df['Corr_Norm']\n",
    "    ) / 3\n",
    "\n",
    "    # Ordina per Consensus Score decrescente\n",
    "    consensus_df = consensus_df.sort_values('Consensus_Score', ascending=False)\n",
    "\n",
    "    print(\"   Consensus Ranking creato\\n\")\n",
    "    print(\" TOP 25 FEATURES BY CONSENSUS RANKING:\\n\")\n",
    "    display_cols = ['Feature', 'Consensus_Score', 'MI_Score', 'F_Statistic', 'Spearman_Corr']\n",
    "    print(consensus_df.head(25)[display_cols].to_string(index=False))\n",
    "    print()\n",
    "\n",
    "    # Salva risultato completo\n",
    "    save_report(consensus_df, '13_consensus_feature_ranking.csv', 'csv')\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"     Errore nella creazione consensus: {e}\")\n",
    "    print(\"   Utilizzare ranking individuali (MI, ANOVA, o Correlation)\")\n",
    "\n",
    "print()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bda306345c6c2bcb",
   "metadata": {},
   "source": [
    "## SEZIONE 14: Domain-Specific Analysis (Network Features) -> NESSUN RISULTATO UTILE - DEPRECATA\n",
    "\n",
    "### Obiettivo\n",
    "\n",
    "Analizzare **feature specifiche del dominio networking** che hanno significato semantico per Intrusion Detection:\n",
    "1. **TCP Flags:** PSH, URG, FIN, SYN, ACK, RST, ECE, CWR\n",
    "2. **Protocol features:** Protocollo trasporto, flow direction\n",
    "3. **Attack signatures:** Pattern noti in letteratura security\n",
    "\n",
    "### TCP Flags: Semantica di Rete\n",
    "\n",
    "I **TCP flags** sono bit nel header TCP che controllano lo stato della connessione:\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "id": "f74eea4cae907ea",
   "metadata": {},
   "source": [
    "# STEP 1: Identificazione Feature Domain-Specific\n",
    "\n",
    "print(\" FASE 12: DOMAIN-SPECIFIC ANALYSIS (Network Features)\")\n",
    "print()\n",
    "\n",
    "# Identifica feature TCP flags (contengono 'Flag' o 'flag' nel nome)\n",
    "tcp_flag_features = [col for col in df_raw.columns if 'Flag' in col or 'flag' in col.lower()]\n",
    "\n",
    "# Identifica feature Protocol\n",
    "protocol_features = [col for col in df_raw.columns if 'Protocol' in col]\n",
    "\n",
    "print(f\" Feature TCP Flags identificate: {len(tcp_flag_features)}\\n\")\n",
    "for feat in tcp_flag_features:\n",
    "    print(f\"   - {feat}\")\n",
    "\n",
    "if protocol_features:\n",
    "    print(f\"\\n Feature Protocol identificate: {len(protocol_features)}\\n\")\n",
    "    for feat in protocol_features:\n",
    "        print(f\"   - {feat}\")\n",
    "\n",
    "print()\n",
    "\n",
    "\n",
    "# STEP 2: Analisi TCP Flags per Attack Type\n",
    "\n",
    "if tcp_flag_features:\n",
    "    print(\"Analisi TCP Flags per Attack Type:\\n\")\n",
    "\n",
    "    # Calcola media per ogni flag per ogni classe\n",
    "    flag_analysis = df_raw.groupby('Label')[tcp_flag_features].mean()\n",
    "\n",
    "    print(\"   Media conteggio TCP Flags per classe:\\n\")\n",
    "    print(flag_analysis.to_string())\n",
    "    print()\n",
    "\n",
    "    # Identifica pattern anomali\n",
    "    print(\"\\nPattern Anomali Identificati:\\n\")\n",
    "\n",
    "    for label in flag_analysis.index:\n",
    "        row = flag_analysis.loc[label]\n",
    "\n",
    "        # Pattern specifici per attack type\n",
    "        if label == 'PortScan':\n",
    "            if 'FIN Flag Count' in row.index and row['FIN Flag Count'] > 0.5:\n",
    "                print(f\"     {label}: Alto FIN count → Possibile FIN scan\")\n",
    "            if 'SYN Flag Count' in row.index and row['SYN Flag Count'] > row.get('ACK Flag Count', 0) * 2:\n",
    "                print(f\"     {label}: SYN >> ACK → SYN scan (incomplete handshake)\")\n",
    "\n",
    "        elif label in ['DoS', 'DDoS']:\n",
    "            if 'SYN Flag Count' in row.index and row['SYN Flag Count'] > 1.0:\n",
    "                print(f\"     {label}: SYN elevato → Possibile SYN flood\")\n",
    "            if 'RST Flag Count' in row.index and row['RST Flag Count'] > 0.5:\n",
    "                print(f\"     {label}: RST elevato → Connection disruption\")\n",
    "\n",
    "        elif label == 'WebAttack':\n",
    "            if 'PSH Flag Count' in row.index and row['PSH Flag Count'] > 0.8:\n",
    "                print(f\"     {label}: PSH elevato → HTTP payload attacks (SQLi, XSS)\")\n",
    "\n",
    "    if not any(['PortScan' in flag_analysis.index, 'DoS' in flag_analysis.index]):\n",
    "        print(\"   Pattern TCP flags coerenti con traffico normale\")\n",
    "\n",
    "    print()\n",
    "\n",
    "\n",
    "# STEP 3: Visualizzazione Heatmap TCP Flags\n",
    "\n",
    "if tcp_flag_features and len(tcp_flag_features) > 0:\n",
    "    print(\"Generazione heatmap TCP Flags per Attack Type...\\n\")\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.heatmap(\n",
    "        flag_analysis.T,  # Transpose: flags su righe, classi su colonne\n",
    "        annot=True,  # Mostra valori numerici\n",
    "        fmt='.2f',  # 2 decimali\n",
    "        cmap='YlOrRd',  # Giallo → Arancione → Rosso\n",
    "        cbar_kws={'label': 'Mean Flag Count'},\n",
    "        linewidths=0.5,\n",
    "        linecolor='gray'\n",
    "    )\n",
    "    plt.title('TCP Flags Average by Attack Type',\n",
    "              fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Attack Type', fontsize=12)\n",
    "    plt.ylabel('TCP Flag', fontsize=12)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(IMG_PATH, '07_tcp_flags_analysis.png'),\n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    print(\"   Heatmap salvata: 07_tcp_flags_analysis.png\")\n",
    "    print()\n",
    "else:\n",
    "    print(\"    Nessuna feature TCP flag disponibile per visualizzazione\")\n",
    "    print()\n",
    "\n",
    "\n",
    "# STEP 4: Analisi Protocol Distribution (se presente)\n",
    "\n",
    "if protocol_features:\n",
    "    print(\"Analisi distribuzione Protocol per Attack Type:\\n\")\n",
    "\n",
    "    for feat in protocol_features:\n",
    "        print(f\"   Feature: {feat}\\n\")\n",
    "\n",
    "        # Distribuzione protocol per classe\n",
    "        protocol_dist = df_raw.groupby('Label')[feat].value_counts(normalize=True).unstack(fill_value=0)\n",
    "\n",
    "        print(protocol_dist.to_string())\n",
    "        print()\n",
    "\n",
    "        # Identifica protocol dominanti per attack\n",
    "        print(\"   Protocol Dominanti per Attack:\\n\")\n",
    "        for label in protocol_dist.index:\n",
    "            top_protocol = protocol_dist.loc[label].idxmax()\n",
    "            top_pct = protocol_dist.loc[label].max() * 100\n",
    "            print(f\"      {label:<20} : {top_protocol} ({top_pct:.1f}%)\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"Nessuna feature Protocol disponibile nel dataset\")\n",
    "    print()\n",
    "\n",
    "print(\"Domain-specific analysis completata\")\n",
    "print()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "af9476224ee45ca4",
   "metadata": {},
   "source": [
    "# ═══════════════════════════════════════════════════════════════\n",
    "# SEZIONE FINALE: SALVATAGGIO DATASET PULITO\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"FASE 16 - SALVATAGGIO DATASET PULITO PER FEATURE ENGINEERING\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "# Prepara dataset per export\n",
    "df_export = df_raw.copy()\n",
    "\n",
    "# SALVA PARQUET (più veloce, consigliato)\n",
    "parquet_path = os.path.join(OUTPUT_PATH, \"cicids2017_cleaned.parquet\")\n",
    "print(f\"Salvataggio Parquet: {parquet_path}\")\n",
    "df_export.to_parquet(parquet_path, index=False, compression='snappy')\n",
    "parquet_size = os.path.getsize(parquet_path) / (1024**2)\n",
    "\n",
    "# SALVA CSV (backup)\n",
    "csv_path = os.path.join(OUTPUT_PATH, \"cicids2017_cleaned.csv\")\n",
    "print(f\"Salvataggio CSV: {csv_path}\")\n",
    "df_export.to_csv(csv_path, index=False)\n",
    "csv_size = os.path.getsize(csv_path) / (1024**2)\n",
    "\n",
    "print()\n",
    "print(\" Dataset cleaned salvato con successo!\")\n",
    "print(f\"   Righe:           {df_export.shape[0]:,}\")\n",
    "print(f\"   Colonne:         {df_export.shape[1]}\")\n",
    "print(f\"   Size Parquet:    {parquet_size:.1f} MB\")\n",
    "print(f\"   Size CSV:        {csv_size:.1f} MB\")\n",
    "print()\n",
    "print(\"Prossimo step: Eseguire FEATURE ENGINEERING per feature selection\")\n",
    "print(\"=\"*80)\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
