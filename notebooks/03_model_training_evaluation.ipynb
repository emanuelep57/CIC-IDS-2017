{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b234d20943d49da8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# NETWORK INTRUSION DETECTION SYSTEM (NIDS)\n",
    "## Fase 3: Machine Learning Models - Training, Evaluation & Comparison\n",
    "\n",
    "\n",
    "## Metodologia:\n",
    "\n",
    "**1. Multi-Model Strategy**\n",
    "- **Random Forest**: Ensemble bagging interpretabile, robustezza dimostrata su CIC-IDS-2017\n",
    "- **LightGBM**: Gradient boosting SOTA, 5-10x più veloce di XGBoost (Ke et al., 2017)\n",
    "- **Neural Network (MLP)**: Deep learning per pattern complessi\n",
    "\n",
    "**2. Imbalance Handling - Double Strategy** (SMOTE, 2002)\n",
    "- **Baseline**: `class_weight='balanced'` nativo, zero overhead computazionale\n",
    "- **Enhanced**: SMOTE con strategia **parziale** inside CV loop (no data leakage)\n",
    "- **Strategia ottimizzata**: SMOTE porta minority a 70% della soglia successiva (non 100%)\n",
    "\n",
    "**3. Cross-Validation Rigorosa**\n",
    "- **StratifiedKFold n_splits=5**: Mantiene proporzioni classi in ogni fold\n",
    "- **SMOTE inside loop**: Applicato SOLO a training fold, MAI a validation fold\n",
    "- **Test set**: Truly held-out, no contamination, no SMOTE\n",
    "- **Critico**: Evita data leakage tra train/validation\n",
    "\n",
    "**4. Metriche Multi-Class**\n",
    "- **Macro F1** (PRIMARY METRIC): Media non pesata, **tratta classi equamente**\n",
    "  - **Perché primaria**: In cybersecurity, OGNI classe (anche rara) è critica \n",
    "- **Weighted F1** (Secondary): Pesato per frequenza, riflette performance globale\n",
    "- **Accuracy**: Informativa ma ingannevole con imbalance (Benign 80% → accuracy inflata)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d71c455f78fe160",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# SECTION 1: Setup Ambiente\n",
    "\n",
    "## Obiettivo\n",
    "Configurare l'ambiente Python con tutte le librerie necessarie per ML training.\n",
    "\n",
    "## Librerie\n",
    "- **scikit-learn 1.x**: Standard per ML\n",
    "- **LightGBM**: Gradient boosting (Ke et al., 2017)\n",
    "- **imbalanced-learn**: SMOTE implementation\n",
    "- **Keras/TensorFlow**: Neural network backend\n",
    "\n",
    "## Riproducibilità\n",
    "- `RANDOM_SEED=42`: Garantisce risultati riproducibili\n",
    "- Fissiamo tutti i random state (numpy, sklearn, lightgbm, keras)\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "bedee1acbe0e6acf",
   "metadata": {},
   "source": [
    "# SECTION 1: SETUP AMBIENTE\n",
    "import sys, pandas as pd, numpy as np\n",
    "import matplotlib.pyplot as plt, seaborn as sns\n",
    "import os, json, warnings, sklearn, time\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix,\n",
    "    accuracy_score, f1_score, precision_score, recall_score\n",
    ")\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Plotting config\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.dpi'] = 120\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.precision', 4)\n",
    "\n",
    "# Riproducibilità\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = '../output/processed_datasets'\n",
    "OUTPUT_DIR = '../output/model_results'\n",
    "IMG_DIR = '../output/images/model_evaluation'\n",
    "\n",
    "for p in [OUTPUT_DIR, IMG_DIR]:\n",
    "    os.makedirs(p, exist_ok=True)\n",
    "\n",
    "print(\" Setup completato.\")\n",
    "print(f\"   Python: {sys.version.split()[0]}\")\n",
    "print(f\"   scikit-learn: {sklearn.__version__}\")\n",
    "print(f\"   LightGBM: {lgb.__version__}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c1bf496b1e88551c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# SECTION 2: Data Loading CON LABEL ENCODING CONSISTENCY\n",
    "\n",
    "# fondamentale: Preservare Label Encoding del Notebook 02\n",
    "\n",
    "## Dataset Split Strategy\n",
    "- **Train**: 70% (1,210,461 samples) - Addestramento modelli\n",
    "- **Validation**: 15% (259,438 samples) - Model selection\n",
    "- **Test**: 15% (259,395 samples) - Final held-out evaluation\n",
    "\n",
    "## Safety Checks\n",
    "1. `Label` NON presente in X (no data leakage)\n",
    "2. Feature identiche in train/val/test\n",
    "3. 20 feature esatte (post feature selection Notebook 02)\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "eef7280da7a2b869",
   "metadata": {},
   "source": [
    "# SECTION 2: DATA LOADING - PRESERVA ENCODING NOTEBOOK 02\n",
    "\n",
    "print(\"Caricamento Dataset Pre-Processati...\")\n",
    "\n",
    "# Load data\n",
    "X_train = pd.read_parquet(os.path.join(DATA_DIR, 'X_train.parquet'))\n",
    "y_train = pd.read_parquet(os.path.join(DATA_DIR, 'y_train.parquet'))['Label']\n",
    "\n",
    "X_val = pd.read_parquet(os.path.join(DATA_DIR, 'X_val.parquet'))\n",
    "y_val = pd.read_parquet(os.path.join(DATA_DIR, 'y_val.parquet'))['Label']\n",
    "\n",
    "X_test = pd.read_parquet(os.path.join(DATA_DIR, 'X_test.parquet'))\n",
    "y_test = pd.read_parquet(os.path.join(DATA_DIR, 'y_test.parquet'))['Label']\n",
    "\n",
    "print(f\"\\nTrain: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
    "\n",
    "#  CRITICO: PRESERVA LABEL ENCODING DEL NOTEBOOK 02\n",
    "le = LabelEncoder()\n",
    "y_train_encoded = le.fit_transform(y_train)\n",
    "y_val_encoded = le.transform(y_val)\n",
    "y_test_encoded = le.transform(y_test)\n",
    "\n",
    "n_classes = len(le.classes_)\n",
    "class_names = le.classes_\n",
    "\n",
    "print(f\"{n_classes} classi: {list(class_names)}\")\n",
    "\n",
    "#  VERIFICA: Stampa mapping per trasparenza\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LABEL ENCODING COERENTE CON NOTEBOOK 02\")\n",
    "print(\"=\"*70)\n",
    "for idx, cls in enumerate(class_names):\n",
    "    print(f\"   {idx} → {cls}\")\n",
    "\n",
    "# Safety Checks\n",
    "assert 'Label' not in X_train.columns\n",
    "assert list(X_train.columns) == list(X_val.columns) == list(X_test.columns)\n",
    "assert X_train.shape[1] == 20\n",
    "print(\"\\n Safety Checks: No leakage, feature allineate.\")\n",
    "\n",
    "# Class Distribution\n",
    "class_counts = pd.Series(y_train_encoded).value_counts().sort_index()\n",
    "class_dist = pd.DataFrame({\n",
    "    'Class': class_names,\n",
    "    'Count': class_counts.values,\n",
    "    'Percentage': (100 * class_counts.values / len(y_train_encoded))\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Distribuzione Classi (Training Set)\")\n",
    "print(\"=\"*70)\n",
    "print(class_dist.to_string(index=False))\n",
    "print(f\"\\nImbalance Ratio: {class_counts.max() / class_counts.min():.0f}:1\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c91d5f76f854f11",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# SECTION 3: Visualizzazione Distribuzione + Strategia SMOTE\n",
    "\n",
    "## Rationale: Class Imbalance in Intrusion Detection\n",
    "\n",
    "\n",
    "## SMOTE Partial Strategy\n",
    "\n",
    "- **Full balancing** (100%): Causa overfitting su synthetic data\n",
    "-  **Partial balancing**: Migliora minority recall senza overfitting\n",
    "\n",
    "## Visualizzazioni\n",
    "- Distribuzione PRE-SMOTE (scala lineare + logaritmica)\n",
    "- Confronto PRE vs POST-SMOTE\n",
    "- Tabella strategia dettagliata\n",
    "\n",
    "\n",
    "### SMOTE Strategy: Gap-Filling 70% - Giustificazione\n",
    "\n",
    "#### Design Principles\n",
    "\n",
    "**Obiettivo**: Bilanciamento progressivo senza oversynthesis\n",
    "\n",
    "#### Safety Constraints\n",
    "\n",
    "1. **Max 20x increment**: Prevent extreme synthetic dominance\n",
    "2. **<95% next class**: Preserve order\n",
    "3. **>5% majority → no SMOTE**: Avoid unnecessary synthesis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "c677ea30221a8176",
   "metadata": {},
   "source": [
    "# SECTION 3: PROGRESSIVE GAP-FILLING SMOTE STRATEGY\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STRATEGIA SMOTE (Progressive Gap-Filling + Order-Preserving)\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "majority_count = class_counts.max()\n",
    "smote_targets = {}\n",
    "\n",
    "print(\" Strategy Principle:\")\n",
    "print(\"   - Fill 70% of the gap between each class and the next larger one\")\n",
    "print(\"   - Guarantees order preservation by design\")\n",
    "print(\"   - Max 20x increment per class (overfitting protection)\")\n",
    "print(\"   - Classes >5% of majority: no change\\n\")\n",
    "\n",
    "# Step 1: Sort classes by original count\n",
    "class_order = sorted([(class_counts[idx], cls, idx) \n",
    "                      for idx, cls in enumerate(class_names)])\n",
    "\n",
    "print(\"Original order (ascending):\")\n",
    "for count, cls, idx in class_order:\n",
    "    print(f\"  {cls:12}: {count:7,} ({count/majority_count*100:6.2f}%)\")\n",
    "print()\n",
    "\n",
    "# Step 2: Progressive gap-filling dal basso verso l'alto\n",
    "for i, (current_count, current_cls, current_idx) in enumerate(class_order):\n",
    "    percent_of_majority = (current_count / majority_count) * 100\n",
    "    \n",
    "    # Classi >5% majority: no change\n",
    "    if percent_of_majority > 5.0:\n",
    "        target_count = current_count\n",
    "        gap_filled = 0\n",
    "        print(f\"  {current_cls:12}: {current_count:7,} → {target_count:7,} \"\n",
    "              f\"(1.0x) [>5% majority - no change]\")\n",
    "    else:\n",
    "        # Trova la classe immediatamente successiva\n",
    "        if i < len(class_order) - 1:\n",
    "            next_count = class_order[i + 1][0]\n",
    "            next_cls = class_order[i + 1][1]\n",
    "            \n",
    "            # Calcola gap\n",
    "            gap = next_count - current_count\n",
    "            \n",
    "            # Target: current + 60% del gap\n",
    "            target_count = current_count + int(gap * 0.70)\n",
    "            \n",
    "            # Safety caps:\n",
    "            # 1. Max 20x increment (overfitting protection)\n",
    "            max_by_increment = current_count * 20\n",
    "            target_count = min(target_count, max_by_increment)\n",
    "            \n",
    "            # 2. Deve rimanere <95% della classe successiva\n",
    "            max_by_order = int(next_count * 0.95)\n",
    "            target_count = min(target_count, max_by_order)\n",
    "            \n",
    "            gap_filled = ((target_count - current_count) / gap * 100) if gap > 0 else 0\n",
    "            \n",
    "            print(f\"  {current_cls:12}: {current_count:7,} → {target_count:7,} \"\n",
    "                  f\"({target_count/current_count:5.1f}x) [gap to {next_cls}: {gap_filled:4.0f}%]\")\n",
    "        else:\n",
    "            # È la classe più grande (Benign)\n",
    "            target_count = current_count\n",
    "            print(f\"  {current_cls:12}: {current_count:7,} → {target_count:7,} \"\n",
    "                  f\"(1.0x) [majority class - no change]\")\n",
    "    \n",
    "    smote_targets[current_cls] = target_count\n",
    "\n",
    "print()\n",
    "\n",
    "# Step 3: Verifica finale ordine\n",
    "print(\"=\"*70)\n",
    "print(\"ORDER VERIFICATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "original_sorted = sorted([(class_counts[idx], cls) for idx, cls in enumerate(class_names)])\n",
    "smote_sorted = sorted([(smote_targets[cls], cls) for cls in class_names])\n",
    "\n",
    "order_preserved = True\n",
    "for i in range(len(class_names)):\n",
    "    orig_count, orig_cls = original_sorted[i]\n",
    "    smote_count, smote_cls = smote_sorted[i]\n",
    "    status = \"\" if orig_cls == smote_cls else \"\"\n",
    "    print(f\"  Position {i+1}: {orig_cls:12} ({orig_count:7,}) → \"\n",
    "          f\"{smote_cls:12} ({smote_count:7,}) {status}\")\n",
    "    if orig_cls != smote_cls:\n",
    "        order_preserved = False\n",
    "\n",
    "if order_preserved:\n",
    "    print(\"\\n PERFECT ORDER PRESERVATION!\\n\")\n",
    "else:\n",
    "    print(\"\\n ORDER VIOLATION DETECTED!\\n\")\n",
    "\n",
    "# Tabella finale\n",
    "strategy_df = pd.DataFrame({\n",
    "    'Class': class_names,\n",
    "    'Original': class_counts.values,\n",
    "    'After SMOTE': [smote_targets[cls] for cls in class_names],\n",
    "    'Increment (x)': [smote_targets[cls] / class_counts[idx]\n",
    "                      for idx, cls in enumerate(class_names)],\n",
    "    '% of Majority (Before)': [(class_counts[idx] / majority_count) * 100\n",
    "                               for idx in range(len(class_names))],\n",
    "    '% of Majority (After)': [(smote_targets[cls] / majority_count) * 100\n",
    "                              for cls in class_names],\n",
    "    'Gap Filled': [((smote_targets[cls] - class_counts[idx]) / \n",
    "                    max(1, class_counts[idx])) * 100 \n",
    "                   for idx, cls in enumerate(class_names)]\n",
    "})\n",
    "\n",
    "strategy_df_sorted = strategy_df.sort_values('Original')\n",
    "print(\"=\"*70)\n",
    "print(\"GAP-FILLING SMOTE STRATEGY (sorted by original count)\")\n",
    "print(\"=\"*70)\n",
    "print(strategy_df_sorted[['Class', 'Original', 'After SMOTE', 'Increment (x)', \n",
    "                          '% of Majority (After)']].to_string(index=False))\n",
    "print(f\"\\nTotal samples: {class_counts.sum():,} → {sum(smote_targets.values()):,}\")\n",
    "print(f\"Increment ratio: {sum(smote_targets.values()) / class_counts.sum():.2f}x\\n\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"STRATEGY JUSTIFICATION\")\n",
    "print(\"=\"*70)\n",
    "print(\" Gap-based approach guarantees order preservation by construction\")\n",
    "print(\" 70% fill ratio balances improvement vs overfitting risk\")\n",
    "print(\" 20x cap prevents synthetic noise on extreme minorities\")\n",
    "print(\" All relative positions maintained exactly as original dataset\")\n",
    "print(\" Computational cost: minimal increase vs aggressive strategies\\n\")\n",
    "\n",
    "# Visualizzazione\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "post_counts = [smote_targets[cls] for cls in class_names]\n",
    "\n",
    "# PRE-SMOTE\n",
    "axes[0].bar(class_names, class_counts.values, color='#5A9FD4', alpha=0.85, \n",
    "            edgecolor='black', linewidth=1.2)\n",
    "axes[0].set_yscale('log')\n",
    "axes[0].set_title('PRE-SMOTE Distribution', fontweight='bold', fontsize=13)\n",
    "axes[0].set_xlabel('Class', fontweight='bold', fontsize=11)\n",
    "axes[0].set_ylabel('Count (log scale)', fontweight='bold', fontsize=11)\n",
    "#  Fix: usa plt.setp invece di tick_params\n",
    "plt.setp(axes[0].get_xticklabels(), rotation=45, ha='right')\n",
    "axes[0].grid(axis='y', alpha=0.3, linestyle='--')\n",
    "\n",
    "# POST-SMOTE\n",
    "axes[1].bar(class_names, post_counts, color='#5CB85C', alpha=0.85, \n",
    "            edgecolor='black', linewidth=1.2)\n",
    "axes[1].set_yscale('log')\n",
    "axes[1].set_title('POST-SMOTE (Gap-Filling Strategy)', fontweight='bold', fontsize=13)\n",
    "axes[1].set_xlabel('Class', fontweight='bold', fontsize=11)\n",
    "axes[1].set_ylabel('Count (log scale)', fontweight='bold', fontsize=11)\n",
    "#  Fix: stessa cosa\n",
    "plt.setp(axes[1].get_xticklabels(), rotation=45, ha='right')\n",
    "axes[1].grid(axis='y', alpha=0.3, linestyle='--')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(IMG_DIR, '01_smote_strategy.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n Gap-filling SMOTE strategy visualization saved\\n\")\n",
    "\n",
    "smote_strategy_config = {\n",
    "    idx: smote_targets[cls]\n",
    "    for idx, cls in enumerate(class_names)\n",
    "}\n",
    "\n",
    "print(f\" SMOTE strategy configurata: {sum(smote_targets.values()):,} samples totali\\n\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "184aec14be2ddae",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# SECTION 4: Model Configuration (Memory-Optimized)\n",
    "\n",
    "#### nota: In cross validation ho usato configurazioni più leggere perché andavo incontro a crash\n",
    "\n",
    "## Hardware Constraints: Ryzen 5 3600X\n",
    "- **CPU**: 6 core / 12 thread @ 3.8GHz\n",
    "- **RAM**: 16GB\n",
    "- **Strategia**: Bilanciare performance vs memory footprint\n",
    "\n",
    "\n",
    "**Rationale 3 modelli**:\n",
    "- RF: Interpretable, robusto, baseline ensemble\n",
    "- LightGBM: SOTA boosting, velocità, performance\n",
    "- MLP: Deep learning, pattern complessi, non-linearità\n",
    "\n",
    "## Pipeline Structure\n",
    "- Nessuno scaling necessario (RF/LightGBM invariant, MLP farà interno)\n",
    "- Pipeline semplice: `classifier` solo"
   ]
  },
  {
   "cell_type": "code",
   "id": "9a9bc517aaac5fdf",
   "metadata": {},
   "source": [
    "# SECTION 4: MODEL CONFIGURATION\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CONFIGURAZIONE MODELLI (Memory-Optimized)\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Random Forest config\n",
    "rf_config = {\n",
    "    'n_estimators': 50,\n",
    "    'max_depth': 20,\n",
    "    'min_samples_split': 10,\n",
    "    'min_samples_leaf': 5,\n",
    "    'class_weight': 'balanced',\n",
    "    'random_state': RANDOM_SEED,\n",
    "    'n_jobs': 4,\n",
    "    'verbose': 0,\n",
    "    'max_samples': 0.8\n",
    "}\n",
    "\n",
    "# LightGBM config\n",
    "lgb_config = {\n",
    "    'n_estimators': 100,\n",
    "    'max_depth': 7,\n",
    "    'learning_rate': 0.05,\n",
    "    'min_child_samples': 50,\n",
    "    'num_leaves': 31,\n",
    "    'is_unbalance': True,\n",
    "    'random_state': RANDOM_SEED,\n",
    "    'n_jobs': 4,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "# MLP config\n",
    "mlp_config = {\n",
    "    'hidden_layer_sizes': (256, 128, 64),\n",
    "    'activation': 'relu',\n",
    "    'solver': 'adam',\n",
    "    'alpha': 0.0001,\n",
    "    'batch_size': 512,\n",
    "    'max_iter': 50,\n",
    "    'early_stopping': True,\n",
    "    'validation_fraction': 0.1,\n",
    "    'random_state': RANDOM_SEED,\n",
    "    'verbose': False\n",
    "}\n",
    "\n",
    "print(\"Random Forest:\")\n",
    "for k, v in rf_config.items():\n",
    "    print(f\"   {k}: {v}\")\n",
    "\n",
    "print(\"\\nLightGBM:\")\n",
    "for k, v in lgb_config.items():\n",
    "    print(f\"   {k}: {v}\")\n",
    "\n",
    "print(\"\\nMLP:\")\n",
    "for k, v in mlp_config.items():\n",
    "    print(f\"   {k}: {v}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a44e5c146ac906c3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# SECTION 4.5: Baseline Cross-Validation (5-Fold)\n",
    "\n",
    "## Metodologia\n",
    "\n",
    "**StratifiedKFold (n_splits=5)**:\n",
    "- Mantiene proporzioni classi in ogni fold\n",
    "- Riduce varianza stime rispetto a single train/val split\n",
    "\n",
    "\n",
    "**Metriche monitorate**:\n",
    "- **Macro F1** (primary): Media non pesata F1 per classe\n",
    "- **Weighted F1**: Pesato per support\n",
    "- **Accuracy**: Overall correct rate\n",
    "\n",
    "**Models trained**:\n",
    "1. Random Forest Baseline (`class_weight='balanced'`)\n",
    "2. LightGBM Baseline (`is_unbalance=True`)\n",
    "3. Neural Network Baseline (no resampling)\n",
    "\n",
    "## Output\n",
    "- Tabella: Model | Macro F1 (mean ± std) | Weighted F1 | Accuracy\n",
    "- Console: Progress log per ogni fold\n",
    "- Tempo training per modello\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "1697261a82bfb092",
   "metadata": {},
   "source": [
    "# SECTION 4.5: BASELINE CROSS-VALIDATION\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BASELINE MODELS - 5-FOLD CROSS-VALIDATION\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "# Metriche da calcolare\n",
    "scoring = {\n",
    "    'macro_f1': 'f1_macro',\n",
    "    'weighted_f1': 'f1_weighted',\n",
    "    'accuracy': 'accuracy'\n",
    "}\n",
    "\n",
    "# Storage results\n",
    "baseline_results = {}\n",
    "\n",
    "# 1. Random Forest Baseline\n",
    "print(\" Random Forest Baseline...\")\n",
    "rf_baseline = RandomForestClassifier(**rf_config)\n",
    "\n",
    "start_time = time.time()\n",
    "cv_results_rf = cross_validate(\n",
    "    rf_baseline, X_train, y_train_encoded,\n",
    "    cv=skf, scoring=scoring, n_jobs=1, verbose=0\n",
    ")\n",
    "train_time_rf = time.time() - start_time\n",
    "\n",
    "baseline_rf_macro = cv_results_rf['test_macro_f1'].mean()\n",
    "baseline_rf_std = cv_results_rf['test_macro_f1'].std()\n",
    "baseline_rf_weighted = cv_results_rf['test_weighted_f1'].mean()\n",
    "baseline_rf_acc = cv_results_rf['test_accuracy'].mean()\n",
    "\n",
    "baseline_results['RF Baseline'] = {\n",
    "    'Macro F1': baseline_rf_macro,\n",
    "    'Std': baseline_rf_std,\n",
    "    'Weighted F1': baseline_rf_weighted,\n",
    "    'Accuracy': baseline_rf_acc,\n",
    "    'Train Time (s)': train_time_rf\n",
    "}\n",
    "\n",
    "print(f\"    Macro F1: {baseline_rf_macro:.4f} ± {baseline_rf_std:.4f}\")\n",
    "print(f\"    Train time: {train_time_rf:.1f}s\\n\")\n",
    "\n",
    "# 2. LightGBM Baseline\n",
    "print(\" LightGBM Baseline...\")\n",
    "lgb_baseline = lgb.LGBMClassifier(**lgb_config)\n",
    "\n",
    "start_time = time.time()\n",
    "cv_results_lgb = cross_validate(\n",
    "    lgb_baseline, X_train, y_train_encoded,\n",
    "    cv=skf, scoring=scoring, n_jobs=1, verbose=0\n",
    ")\n",
    "train_time_lgb = time.time() - start_time\n",
    "\n",
    "baseline_lgb_macro = cv_results_lgb['test_macro_f1'].mean()\n",
    "baseline_lgb_std = cv_results_lgb['test_macro_f1'].std()\n",
    "baseline_lgb_weighted = cv_results_lgb['test_weighted_f1'].mean()\n",
    "baseline_lgb_acc = cv_results_lgb['test_accuracy'].mean()\n",
    "\n",
    "baseline_results['LGB Baseline'] = {\n",
    "    'Macro F1': baseline_lgb_macro,\n",
    "    'Std': baseline_lgb_std,\n",
    "    'Weighted F1': baseline_lgb_weighted,\n",
    "    'Accuracy': baseline_lgb_acc,\n",
    "    'Train Time (s)': train_time_lgb\n",
    "}\n",
    "\n",
    "print(f\"    Macro F1: {baseline_lgb_macro:.4f} ± {baseline_lgb_std:.4f}\")\n",
    "print(f\"    Train time: {train_time_lgb:.1f}s\\n\")\n",
    "\n",
    "# 3. MLP Baseline (con StandardScaler)\n",
    "print(\" Neural Network Baseline...\")\n",
    "mlp_baseline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('mlp', MLPClassifier(**mlp_config))\n",
    "])\n",
    "\n",
    "start_time = time.time()\n",
    "cv_results_mlp = cross_validate(\n",
    "    mlp_baseline, X_train, y_train_encoded,\n",
    "    cv=skf, scoring=scoring, n_jobs=1, verbose=0\n",
    ")\n",
    "train_time_mlp = time.time() - start_time\n",
    "\n",
    "baseline_mlp_macro = cv_results_mlp['test_macro_f1'].mean()\n",
    "baseline_mlp_std = cv_results_mlp['test_macro_f1'].std()\n",
    "baseline_mlp_weighted = cv_results_mlp['test_weighted_f1'].mean()\n",
    "baseline_mlp_acc = cv_results_mlp['test_accuracy'].mean()\n",
    "\n",
    "baseline_results['MLP Baseline'] = {\n",
    "    'Macro F1': baseline_mlp_macro,\n",
    "    'Std': baseline_mlp_std,\n",
    "    'Weighted F1': baseline_mlp_weighted,\n",
    "    'Accuracy': baseline_mlp_acc,\n",
    "    'Train Time (s)': train_time_mlp\n",
    "}\n",
    "\n",
    "print(f\"    Macro F1: {baseline_mlp_macro:.4f} ± {baseline_mlp_std:.4f}\")\n",
    "print(f\"    Train time: {train_time_mlp:.1f}s\\n\")\n",
    "\n",
    "# Tabella risultati\n",
    "baseline_df = pd.DataFrame(baseline_results).T\n",
    "baseline_df['Macro F1 (±std)'] = baseline_df.apply(\n",
    "    lambda row: f\"{row['Macro F1']:.4f} ± {row['Std']:.4f}\", axis=1\n",
    ")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"BASELINE CV RESULTS (5-Fold)\")\n",
    "print(\"=\"*70)\n",
    "print(baseline_df[['Macro F1 (±std)', 'Weighted F1', 'Accuracy', 'Train Time (s)']].to_string())\n",
    "print(\"\\n Baseline CV completata\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a7de574b0822c376",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# SECTION 4.6: SMOTE Cross-Validation (5-Fold)\n",
    "\n",
    "## CRITICAL: SMOTE Inside CV Loop\n",
    "\n",
    "**Metodologia rigorosa** (no data leakage):\n",
    "```python\n",
    "for train_idx, val_idx in CV_folds:\n",
    "    X_train_fold = X_train[train_idx]\n",
    "    X_val_fold = X_train[val_idx]\n",
    "\n",
    "    #  SMOTE solo su train fold\n",
    "    X_train_smote, y_train_smote = SMOTE().fit_resample(X_train_fold, y_train_fold)\n",
    "\n",
    "    #  Validation fold: NO SMOTE\n",
    "    model.fit(X_train_smote, y_train_smote)\n",
    "    score = model.score(X_val_fold, y_val_fold)  # Originale, no synthetic\n",
    "```\n",
    "\n",
    "Perché CRITICO:\n",
    "\n",
    "Applicare SMOTE a intero train+val → data leakage (synthetic contamina validation)\n",
    "\n",
    "Validation deve simulare test set (real data only)\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "bc0acb0cc9c313ce",
   "metadata": {},
   "source": [
    "\n",
    "# SECTION 4.6: SMOTE CROSS-VALIDATION (INSIDE LOOP)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SMOTE MODELS - 5-FOLD CROSS-VALIDATION (Inside Loop)\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Storage results\n",
    "smote_results = {}\n",
    "\n",
    "# Helper function: Manual CV con SMOTE inside loop\n",
    "def smote_cv(model, X, y, cv, smote_strategy_config):\n",
    "    \"\"\"Cross-validation con SMOTE applicato solo a train fold\"\"\"\n",
    "    macro_f1_scores = []\n",
    "    weighted_f1_scores = []\n",
    "    accuracy_scores = []\n",
    "\n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(cv.split(X, y)):\n",
    "        # Split fold\n",
    "        X_train_fold = X.iloc[train_idx]\n",
    "        y_train_fold = y[train_idx]\n",
    "        X_val_fold = X.iloc[val_idx]\n",
    "        y_val_fold = y[val_idx]\n",
    "\n",
    "        #  Apply SMOTE SOLO a train fold\n",
    "        smote = SMOTE(sampling_strategy=smote_strategy_config, random_state=RANDOM_SEED)\n",
    "        X_train_smote, y_train_smote = smote.fit_resample(X_train_fold, y_train_fold)\n",
    "\n",
    "        # Train model\n",
    "        model.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "        #  Evaluate on ORIGINAL validation fold (no SMOTE)\n",
    "        y_val_pred = model.predict(X_val_fold)\n",
    "\n",
    "        macro_f1_scores.append(f1_score(y_val_fold, y_val_pred, average='macro'))\n",
    "        weighted_f1_scores.append(f1_score(y_val_fold, y_val_pred, average='weighted'))\n",
    "        accuracy_scores.append(accuracy_score(y_val_fold, y_val_pred))\n",
    "\n",
    "    return {\n",
    "        'macro_f1': np.array(macro_f1_scores),\n",
    "        'weighted_f1': np.array(weighted_f1_scores),\n",
    "        'accuracy': np.array(accuracy_scores)\n",
    "    }\n",
    "\n",
    "# 1. Random Forest + SMOTE\n",
    "print(\" Random Forest + SMOTE...\")\n",
    "rf_smote = RandomForestClassifier(**rf_config)\n",
    "\n",
    "start_time = time.time()\n",
    "cv_results_rf_smote = smote_cv(rf_smote, X_train, y_train_encoded, skf, smote_strategy_config)\n",
    "train_time_rf_smote = time.time() - start_time\n",
    "\n",
    "smote_rf_macro = cv_results_rf_smote['macro_f1'].mean()\n",
    "smote_rf_std = cv_results_rf_smote['macro_f1'].std()\n",
    "smote_rf_weighted = cv_results_rf_smote['weighted_f1'].mean()\n",
    "smote_rf_acc = cv_results_rf_smote['accuracy'].mean()\n",
    "\n",
    "smote_results['RF SMOTE'] = {\n",
    "    'Macro F1': smote_rf_macro,\n",
    "    'Std': smote_rf_std,\n",
    "    'Weighted F1': smote_rf_weighted,\n",
    "    'Accuracy': smote_rf_acc,\n",
    "    'Train Time (s)': train_time_rf_smote\n",
    "}\n",
    "\n",
    "print(f\"    Macro F1: {smote_rf_macro:.4f} ± {smote_rf_std:.4f}\")\n",
    "print(f\"    Train time: {train_time_rf_smote:.1f}s\\n\")\n",
    "\n",
    "# 2. LightGBM + SMOTE\n",
    "print(\" LightGBM + SMOTE...\")\n",
    "lgb_smote = lgb.LGBMClassifier(**lgb_config)\n",
    "\n",
    "start_time = time.time()\n",
    "cv_results_lgb_smote = smote_cv(lgb_smote, X_train, y_train_encoded, skf, smote_strategy_config)\n",
    "train_time_lgb_smote = time.time() - start_time\n",
    "\n",
    "smote_lgb_macro = cv_results_lgb_smote['macro_f1'].mean()\n",
    "smote_lgb_std = cv_results_lgb_smote['macro_f1'].std()\n",
    "smote_lgb_weighted = cv_results_lgb_smote['weighted_f1'].mean()\n",
    "smote_lgb_acc = cv_results_lgb_smote['accuracy'].mean()\n",
    "\n",
    "smote_results['LGB SMOTE'] = {\n",
    "    'Macro F1': smote_lgb_macro,\n",
    "    'Std': smote_lgb_std,\n",
    "    'Weighted F1': smote_lgb_weighted,\n",
    "    'Accuracy': smote_lgb_acc,\n",
    "    'Train Time (s)': train_time_lgb_smote\n",
    "}\n",
    "\n",
    "print(f\"    Macro F1: {smote_lgb_macro:.4f} ± {smote_lgb_std:.4f}\")\n",
    "print(f\"    Train time: {train_time_lgb_smote:.1f}s\\n\")\n",
    "\n",
    "# 3. MLP + SMOTE\n",
    "print(\" Neural Network + SMOTE...\")\n",
    "# Per MLP, dobbiamo applicare StandardScaler DOPO SMOTE\n",
    "def smote_cv_mlp(mlp_config, X, y, cv, smote_strategy_config):\n",
    "    \"\"\"CV MLP con SMOTE + StandardScaler\"\"\"\n",
    "    macro_f1_scores = []\n",
    "    weighted_f1_scores = []\n",
    "    accuracy_scores = []\n",
    "\n",
    "    for train_idx, val_idx in cv.split(X, y):\n",
    "        X_train_fold = X.iloc[train_idx]\n",
    "        y_train_fold = y[train_idx]\n",
    "        X_val_fold = X.iloc[val_idx]\n",
    "        y_val_fold = y[val_idx]\n",
    "\n",
    "        # SMOTE\n",
    "        smote = SMOTE(sampling_strategy=smote_strategy_config, random_state=RANDOM_SEED)\n",
    "        X_train_smote, y_train_smote = smote.fit_resample(X_train_fold, y_train_fold)\n",
    "\n",
    "        # StandardScaler DOPO SMOTE\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train_smote)\n",
    "        X_val_scaled = scaler.transform(X_val_fold)\n",
    "\n",
    "        # Train MLP\n",
    "        mlp = MLPClassifier(**mlp_config)\n",
    "        mlp.fit(X_train_scaled, y_train_smote)\n",
    "\n",
    "        # Evaluate\n",
    "        y_val_pred = mlp.predict(X_val_scaled)\n",
    "        macro_f1_scores.append(f1_score(y_val_fold, y_val_pred, average='macro'))\n",
    "        weighted_f1_scores.append(f1_score(y_val_fold, y_val_pred, average='weighted'))\n",
    "        accuracy_scores.append(accuracy_score(y_val_fold, y_val_pred))\n",
    "\n",
    "    return {\n",
    "        'macro_f1': np.array(macro_f1_scores),\n",
    "        'weighted_f1': np.array(weighted_f1_scores),\n",
    "        'accuracy': np.array(accuracy_scores)\n",
    "    }\n",
    "\n",
    "start_time = time.time()\n",
    "cv_results_mlp_smote = smote_cv_mlp(mlp_config, X_train, y_train_encoded, skf, smote_strategy_config)\n",
    "train_time_mlp_smote = time.time() - start_time\n",
    "\n",
    "smote_mlp_macro = cv_results_mlp_smote['macro_f1'].mean()\n",
    "smote_mlp_std = cv_results_mlp_smote['macro_f1'].std()\n",
    "smote_mlp_weighted = cv_results_mlp_smote['weighted_f1'].mean()\n",
    "smote_mlp_acc = cv_results_mlp_smote['accuracy'].mean()\n",
    "\n",
    "smote_results['MLP SMOTE'] = {\n",
    "    'Macro F1': smote_mlp_macro,\n",
    "    'Std': smote_mlp_std,\n",
    "    'Weighted F1': smote_mlp_weighted,\n",
    "    'Accuracy': smote_mlp_acc,\n",
    "    'Train Time (s)': train_time_mlp_smote\n",
    "}\n",
    "\n",
    "print(f\"    Macro F1: {smote_mlp_macro:.4f} ± {smote_mlp_std:.4f}\")\n",
    "print(f\"    Train time: {train_time_mlp_smote:.1f}s\\n\")\n",
    "\n",
    "# Tabella risultati\n",
    "smote_df = pd.DataFrame(smote_results).T\n",
    "smote_df['Macro F1 (±std)'] = smote_df.apply(\n",
    "    lambda row: f\"{row['Macro F1']:.4f} ± {row['Std']:.4f}\", axis=1\n",
    ")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"SMOTE CV RESULTS (5-Fold, Inside Loop)\")\n",
    "print(\"=\"*70)\n",
    "print(smote_df[['Macro F1 (±std)', 'Weighted F1', 'Accuracy', 'Train Time (s)']].to_string())\n",
    "print(\"\\n SMOTE CV completata\")\n",
    "\n",
    "# Salva CSV\n",
    "cv_comparison = pd.concat([baseline_df, smote_df])\n",
    "cv_comparison.to_csv(os.path.join(OUTPUT_DIR, 'cv_results_baseline_vs_smote.csv'))\n",
    "print(\"\\n Risultati CV salvati: cv_results_baseline_vs_smote.csv\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4b12a28ffa8b5315",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# SECTION 5: Model Selection on Validation Set\n",
    "\n",
    "## Cosa facciamo in questa sezione\n",
    "\n",
    "- Alleniamo **6 modelli**:\n",
    "  - 3 baseline: RF, LightGBM, MLP\n",
    "  - 3 SMOTE: RF+SMOTE, LGB+SMOTE, MLP+SMOTE\n",
    "- Valutiamo ognuno sul **validation set**:\n",
    "  - Macro F1 (primary)\n",
    "  - Weighted F1\n",
    "  - Accuracy\n",
    "- Selezioniamo il **best model** (Macro F1 massimo)\n",
    "- Salviamo:\n",
    "  - `validation_model_selection.csv`\n",
    "  - `05_validation_model_selection.png` (bar chart baseline vs SMOTE)\n",
    "\n",
    "---\n",
    "\n",
    "### Interpretazione – Validation Results\n",
    "\n",
    "**Best performing model:**\n",
    "- [identificare modello con F1-macro più alto]\n",
    "- Possibili motivi: [es. SMOTE aiuta?, tree-based vs neural?]\n",
    "\n",
    "**SMOTE Impact:**\n",
    "- Confronta baseline vs SMOTE per ogni algoritmo\n",
    "- SMOTE migliora recall su classi minoritarie? Trade-off con precision?\n",
    "\n",
    "**Algorithm Comparison:**\n",
    "- Random Forest vs LightGBM: quale performa meglio?\n",
    "- MLP competitivo? Considerazioni: tempo training, interpretabilità\n",
    "\n",
    "**Next Steps:**\n",
    "- Analisi dettagliata per-model (Section 6)\n",
    "- Confusion matrix, feature importance, misclassification\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "6864b143a6f6ba6c",
   "metadata": {},
   "source": [
    "# SECTION 5: MODEL CONFIGURATIONS\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SECTION 5: MODEL TRAINING & VALIDATION\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Configurazioni modelli\n",
    "rf_config = {\n",
    "    'n_estimators': 200,\n",
    "    'max_depth': 25,\n",
    "    'min_samples_split': 5,\n",
    "    'min_samples_leaf': 2,\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1,\n",
    "    'class_weight': 'balanced'  # gestisce imbalance internamente\n",
    "}\n",
    "\n",
    "lgb_config = {\n",
    "    'n_estimators': 200,\n",
    "    'max_depth': 25,\n",
    "    'learning_rate': 0.1,\n",
    "    'num_leaves': 50,\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1,\n",
    "    'verbose': -1,\n",
    "    'class_weight': 'balanced'\n",
    "}\n",
    "\n",
    "mlp_config_dict = {\n",
    "    'hidden_layers': [256, 128, 64],\n",
    "    'dropout_rate': 0.3,\n",
    "    'learning_rate': 0.001,\n",
    "    'batch_size': 64,\n",
    "    'epochs': 100,\n",
    "    'patience': 15\n",
    "}\n",
    "\n",
    "print(\" Configurazioni definite\\n\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2c61766f18ba1306",
   "metadata": {},
   "source": [
    "# SECTION 5.1: DATA SCALING (necessario per MLP)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(X_train)\n",
    "x_val_scaled = scaler.transform(X_val)\n",
    "x_test_scaled = scaler.transform(X_test)  # scala anche test per uso futuro\n",
    "\n",
    "print(\" Feature scaling completato\")\n",
    "print(f\"  Train shape: {x_train_scaled.shape}\")\n",
    "print(f\"  Val shape:   {x_val_scaled.shape}\")\n",
    "print(f\"  Test shape:  {x_test_scaled.shape}\\n\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ac7bc4d372678477",
   "metadata": {},
   "source": [
    "# SECTION 5.2: BASELINE MODELS (no SMOTE)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import lightgbm as lgb\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"TRAINING BASELINE MODELS (no balancing)\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# ========== 1) RANDOM FOREST BASELINE ==========\n",
    "print(\"[1/3] Training Random Forest Baseline...\")\n",
    "rf_baseline = RandomForestClassifier(**rf_config)\n",
    "rf_baseline.fit(X_train, y_train_encoded)\n",
    "\n",
    "val_pred_rf = rf_baseline.predict(X_val)\n",
    "acc_rf_val = accuracy_score(y_val_encoded, val_pred_rf)\n",
    "f1_rf_val = f1_score(y_val_encoded, val_pred_rf, average='macro')\n",
    "\n",
    "print(f\"   RF Baseline trained\")\n",
    "print(f\"    Validation Accuracy: {acc_rf_val:.4f}\")\n",
    "print(f\"    Validation F1-Macro: {f1_rf_val:.4f}\\n\")\n",
    "\n",
    "# ========== 2) LIGHTGBM BASELINE ==========\n",
    "print(\"[2/3] Training LightGBM Baseline...\")\n",
    "lgb_baseline = lgb.LGBMClassifier(**lgb_config)\n",
    "lgb_baseline.fit(X_train, y_train_encoded)\n",
    "\n",
    "val_pred_lgb = lgb_baseline.predict(X_val)\n",
    "acc_lgb_val = accuracy_score(y_val_encoded, val_pred_lgb)\n",
    "f1_lgb_val = f1_score(y_val_encoded, val_pred_lgb, average='macro')\n",
    "\n",
    "print(f\"   LGB Baseline trained\")\n",
    "print(f\"    Validation Accuracy: {acc_lgb_val:.4f}\")\n",
    "print(f\"    Validation F1-Macro: {f1_lgb_val:.4f}\\n\")\n",
    "\n",
    "# ========== 3) MLP BASELINE ==========\n",
    "print(\"[3/3] Training MLP Baseline...\")\n",
    "\n",
    "# Build model\n",
    "mlp_baseline = keras.Sequential()\n",
    "mlp_baseline.add(layers.Input(shape=(X_train.shape[1],)))\n",
    "\n",
    "for units in mlp_config_dict['hidden_layers']:\n",
    "    mlp_baseline.add(layers.Dense(units, activation='relu'))\n",
    "    mlp_baseline.add(layers.Dropout(mlp_config_dict['dropout_rate']))\n",
    "\n",
    "mlp_baseline.add(layers.Dense(len(class_names), activation='softmax'))\n",
    "\n",
    "mlp_baseline.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=mlp_config_dict['learning_rate']),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=mlp_config_dict['patience'],\n",
    "    restore_best_weights=True,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "history_mlp = mlp_baseline.fit(\n",
    "    x_train_scaled, y_train_encoded,\n",
    "    validation_data=(x_val_scaled, y_val_encoded),\n",
    "    epochs=mlp_config_dict['epochs'],\n",
    "    batch_size=mlp_config_dict['batch_size'],\n",
    "    callbacks=[early_stop],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "val_pred_mlp_proba = mlp_baseline.predict(x_val_scaled, verbose=0)\n",
    "val_pred_mlp = np.argmax(val_pred_mlp_proba, axis=1)\n",
    "acc_mlp_val = accuracy_score(y_val_encoded, val_pred_mlp)\n",
    "f1_mlp_val = f1_score(y_val_encoded, val_pred_mlp, average='macro')\n",
    "\n",
    "print(f\"   MLP Baseline trained ({len(history_mlp.history['loss'])} epochs)\")\n",
    "print(f\"    Validation Accuracy: {acc_mlp_val:.4f}\")\n",
    "print(f\"    Validation F1-Macro: {f1_mlp_val:.4f}\\n\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"BASELINE MODELS TRAINING COMPLETED\")\n",
    "print(\"=\"*70 + \"\\n\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b15708810c15fc55",
   "metadata": {},
   "source": [
    "# SECTION 5.3: SMOTE MODELS\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"TRAINING SMOTE MODELS (balanced training set)\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Applica SMOTE al training set\n",
    "print(\"Applying SMOTE to training data...\")\n",
    "\n",
    "smote_sampler = SMOTE(sampling_strategy=smote_strategy_config, random_state=42)\n",
    "x_train_smote, y_train_smote = smote_sampler.fit_resample(X_train, y_train_encoded)\n",
    "\n",
    "\n",
    "print(f\"  Original training samples: {X_train.shape[0]}\")\n",
    "print(f\"  SMOTE training samples:    {x_train_smote.shape[0]}\")\n",
    "print(f\"   SMOTE applied\\n\")\n",
    "\n",
    "# Scala versione SMOTE per MLP\n",
    "x_train_smote_scaled = scaler.fit_transform(x_train_smote)\n",
    "\n",
    "# ========== 4) RANDOM FOREST + SMOTE ==========\n",
    "print(\"[1/3] Training Random Forest + SMOTE...\")\n",
    "# Rimuovi class_weight (SMOTE già bilancia)\n",
    "rf_smote_config = {k: v for k, v in rf_config.items() if k != 'class_weight'}\n",
    "rf_smote = RandomForestClassifier(**rf_smote_config)\n",
    "rf_smote.fit(x_train_smote, y_train_smote)\n",
    "\n",
    "val_pred_rf_smote = rf_smote.predict(X_val)\n",
    "acc_rf_smote_val = accuracy_score(y_val_encoded, val_pred_rf_smote)\n",
    "f1_rf_smote_val = f1_score(y_val_encoded, val_pred_rf_smote, average='macro')\n",
    "\n",
    "print(f\"   RF + SMOTE trained\")\n",
    "print(f\"    Validation Accuracy: {acc_rf_smote_val:.4f}\")\n",
    "print(f\"    Validation F1-Macro: {f1_rf_smote_val:.4f}\\n\")\n",
    "\n",
    "# ========== 5) LIGHTGBM + SMOTE ==========\n",
    "print(\"[2/3] Training LightGBM + SMOTE...\")\n",
    "lgb_smote_config = {k: v for k, v in lgb_config.items() if k != 'class_weight'}\n",
    "lgb_smote = lgb.LGBMClassifier(**lgb_smote_config)\n",
    "lgb_smote.fit(x_train_smote, y_train_smote)\n",
    "\n",
    "val_pred_lgb_smote = lgb_smote.predict(X_val)\n",
    "acc_lgb_smote_val = accuracy_score(y_val_encoded, val_pred_lgb_smote)\n",
    "f1_lgb_smote_val = f1_score(y_val_encoded, val_pred_lgb_smote, average='macro')\n",
    "\n",
    "print(f\"   LGB + SMOTE trained\")\n",
    "print(f\"    Validation Accuracy: {acc_lgb_smote_val:.4f}\")\n",
    "print(f\"    Validation F1-Macro: {f1_lgb_smote_val:.4f}\\n\")\n",
    "\n",
    "# ========== 6) MLP + SMOTE ==========\n",
    "print(\"[3/3] Training MLP + SMOTE...\")\n",
    "\n",
    "mlp_smote = keras.Sequential()\n",
    "mlp_smote.add(layers.Input(shape=(x_train_smote.shape[1],)))\n",
    "\n",
    "for units in mlp_config_dict['hidden_layers']:\n",
    "    mlp_smote.add(layers.Dense(units, activation='relu'))\n",
    "    mlp_smote.add(layers.Dropout(mlp_config_dict['dropout_rate']))\n",
    "\n",
    "mlp_smote.add(layers.Dense(len(class_names), activation='softmax'))\n",
    "\n",
    "mlp_smote.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=mlp_config_dict['learning_rate']),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "early_stop_smote = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=mlp_config_dict['patience'],\n",
    "    restore_best_weights=True,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "history_mlp_smote = mlp_smote.fit(\n",
    "    x_train_smote_scaled, y_train_smote,\n",
    "    validation_data=(x_val_scaled, y_val_encoded),\n",
    "    epochs=mlp_config_dict['epochs'],\n",
    "    batch_size=mlp_config_dict['batch_size'],\n",
    "    callbacks=[early_stop_smote],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "val_pred_mlp_smote_proba = mlp_smote.predict(x_val_scaled, verbose=0)\n",
    "val_pred_mlp_smote = np.argmax(val_pred_mlp_smote_proba, axis=1)\n",
    "acc_mlp_smote_val = accuracy_score(y_val_encoded, val_pred_mlp_smote)\n",
    "f1_mlp_smote_val = f1_score(y_val_encoded, val_pred_mlp_smote, average='macro')\n",
    "\n",
    "print(f\"   MLP + SMOTE trained ({len(history_mlp_smote.history['loss'])} epochs)\")\n",
    "print(f\"    Validation Accuracy: {acc_mlp_smote_val:.4f}\")\n",
    "print(f\"    Validation F1-Macro: {f1_mlp_smote_val:.4f}\\n\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"SMOTE MODELS TRAINING COMPLETED\")\n",
    "print(\"=\"*70 + \"\\n\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ffd8579efb9d1c75",
   "metadata": {},
   "source": [
    "# SECTION 5.4: VALIDATION RESULTS COMPARISON (Fixed)\n",
    "\n",
    "results_val = pd.DataFrame({\n",
    "    'Model': [\n",
    "        'RF_Baseline', 'LGB_Baseline', 'MLP_Baseline',\n",
    "        'RF_SMOTE', 'LGB_SMOTE', 'MLP_SMOTE'\n",
    "    ],\n",
    "    'Validation_Accuracy': [\n",
    "        acc_rf_val, acc_lgb_val, acc_mlp_val,\n",
    "        acc_rf_smote_val, acc_lgb_smote_val, acc_mlp_smote_val\n",
    "    ],\n",
    "    'Validation_F1_Macro': [\n",
    "        f1_rf_val, f1_lgb_val, f1_mlp_val,\n",
    "        f1_rf_smote_val, f1_lgb_smote_val, f1_mlp_smote_val\n",
    "    ]\n",
    "})\n",
    "\n",
    "results_val = results_val.sort_values('Validation_F1_Macro', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"VALIDATION RESULTS SUMMARY\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "print(results_val.to_string(index=False))\n",
    "\n",
    "results_val.to_csv(os.path.join(OUTPUT_DIR, '05_validation_results_comparison.csv'), index=False)\n",
    "print(\"\\n Risultati salvati in:\", os.path.join(OUTPUT_DIR, '05_validation_results_comparison.csv'))\n",
    "\n",
    "# Plot comparison (Fixed: no xlim restriction, better colors)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Accuracy - colori uniformi\n",
    "colors_acc = ['#2E86AB', '#A23B72', '#F18F01', '#2E86AB', '#A23B72', '#F18F01']\n",
    "axes[0].barh(results_val['Model'], results_val['Validation_Accuracy'],\n",
    "             color=colors_acc, alpha=0.85, edgecolor='black')\n",
    "axes[0].set_xlabel('Validation Accuracy', fontweight='bold', fontsize=11)\n",
    "axes[0].set_title('Model Comparison: Validation Accuracy', fontweight='bold', fontsize=12)\n",
    "axes[0].grid(axis='x', alpha=0.3, linestyle='--')\n",
    "# Rimuovi xlim fisso: lascia che si adatti ai dati\n",
    "axes[0].set_xlim([results_val['Validation_Accuracy'].min() - 0.05, 1.0])\n",
    "\n",
    "# F1-Macro\n",
    "axes[1].barh(results_val['Model'], results_val['Validation_F1_Macro'],\n",
    "             color=colors_acc, alpha=0.85, edgecolor='black')\n",
    "axes[1].set_xlabel('Validation F1-Macro', fontweight='bold', fontsize=11)\n",
    "axes[1].set_title('Model Comparison: Validation F1-Macro', fontweight='bold', fontsize=12)\n",
    "axes[1].grid(axis='x', alpha=0.3, linestyle='--')\n",
    "axes[1].set_xlim([results_val['Validation_F1_Macro'].min() - 0.05, 1.0])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(IMG_DIR, '05_validation_results_comparison.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n Plot salvato\\n\")\n",
    "print(\"=\"*70)\n",
    "print(\"SECTION 5 COMPLETED\")\n",
    "print(\"=\"*70 + \"\\n\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d7ea24dde3678ea9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# SECTION 6: Detailed Per-Model Analysis\n",
    "\n",
    "## Obiettivo\n",
    "\n",
    "Analizzare in dettaglio il comportamento di OGNI modello (6 totali) sul **validation set**:\n",
    "\n",
    "- Confusion Matrix annotata (8x8)\n",
    "- Per-class metrics table (Precision, Recall, F1, Support)\n",
    "- Feature importance (solo RF/LightGBM)\n",
    "- Misclassification analysis (Top-10 errori più frequenti)\n",
    "- Commento testuale (interpretazione risultati)\n",
    "\n",
    "## Struttura\n",
    "\n",
    "- 6.1: Random Forest Baseline\n",
    "- 6.2: LightGBM Baseline\n",
    "- 6.3: MLP Baseline\n",
    "- 6.4: Random Forest + SMOTE\n",
    "- 6.5: LightGBM + SMOTE\n",
    "- 6.6: MLP + SMOTE\n",
    "\n",
    "Tutta l’analisi è fatta su **validation set** (no test leakage).\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "fdc1f2a53dff72fa",
   "metadata": {},
   "source": [
    "# COLORMAP PERSONALIZZATA per tutte le confusion matrix\n",
    "# Usa una scala da bianco → blu scuro per leggibilità ottimale\n",
    "\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "# Crea colormap personalizzata: bianco → azzurro → blu scuro\n",
    "colors_cm = ['#FFFFFF', '#E3F2FD', '#90CAF9', '#42A5F5', '#1E88E5', '#1565C0', '#0D47A1']\n",
    "n_bins = 100\n",
    "cmap_custom = mcolors.LinearSegmentedColormap.from_list('custom_blues', colors_cm, N=n_bins)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "13c3a3e99df8d994",
   "metadata": {},
   "source": [
    "# SECTION 6.1: RANDOM FOREST BASELINE – DETAILED ANALYSIS\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SECTION 6.1: RANDOM FOREST BASELINE – DETAILED ANALYSIS\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# 1) Confusion Matrix\n",
    "cm_rf = confusion_matrix(y_val_encoded, val_pred_rf)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "sns.heatmap(\n",
    "    cm_rf, annot=True, fmt='d', cmap=cmap_custom,\n",
    "    xticklabels=class_names, yticklabels=class_names,\n",
    "    cbar_kws={'label': 'Count'}, ax=ax,\n",
    "    linewidths=0.5, linecolor='gray',\n",
    "    annot_kws={'fontsize': 9}\n",
    ")\n",
    "ax.set_xlabel('Predicted Label', fontweight='bold', fontsize=11)\n",
    "ax.set_ylabel('True Label', fontweight='bold', fontsize=11)\n",
    "ax.set_title('Random Forest Baseline – Confusion Matrix (Validation Set)',\n",
    "             fontweight='bold', fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(IMG_DIR, '06_1_rf_baseline_confusion_matrix.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\" Confusion matrix salvata\\n\")\n",
    "\n",
    "# 2) Per-class metrics\n",
    "report_rf = classification_report(\n",
    "    y_val_encoded, val_pred_rf,\n",
    "    target_names=class_names,\n",
    "    output_dict=True\n",
    ")\n",
    "perclass_rf = pd.DataFrame(report_rf).T.iloc[:-3, :]\n",
    "perclass_rf['support'] = perclass_rf['support'].astype(int)\n",
    "perclass_rf.index.name = 'Class'\n",
    "\n",
    "print(\"Per-Class Metrics (RF Baseline):\")\n",
    "print(perclass_rf[['precision', 'recall', 'f1-score', 'support']].to_string())\n",
    "\n",
    "perclass_rf.to_csv(os.path.join(OUTPUT_DIR, '06_1_rf_baseline_perclass_metrics.csv'))\n",
    "print(\"\\n Per-class metrics salvate\\n\")\n",
    "\n",
    "# 3) Feature importance (Top-20)\n",
    "fi_rf = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': rf_baseline.feature_importances_\n",
    "}).sort_values('Importance', ascending=False).head(20)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "ax.barh(fi_rf['Feature'], fi_rf['Importance'], color='steelblue', alpha=0.9)\n",
    "ax.invert_yaxis()\n",
    "ax.set_xlabel('Gini Importance', fontweight='bold')\n",
    "ax.set_title('Random Forest Baseline – Top 20 Features by Importance', fontweight='bold')\n",
    "ax.grid(axis='x', linestyle='--', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(IMG_DIR, '06_1_rf_baseline_feature_importance.png'), dpi=150)\n",
    "plt.show()\n",
    "print(\" Feature importance salvata\\n\")\n",
    "\n",
    "fi_rf.to_csv(os.path.join(OUTPUT_DIR, '06_1_rf_baseline_feature_importance.csv'), index=False)\n",
    "\n",
    "# 4) Misclassification analysis (Top-10 errori)\n",
    "misclass_rf = []\n",
    "for true_idx in range(len(class_names)):\n",
    "    for pred_idx in range(len(class_names)):\n",
    "        if true_idx != pred_idx:\n",
    "            count = cm_rf[true_idx, pred_idx]\n",
    "            if count > 0:\n",
    "                misclass_rf.append({\n",
    "                    'True_Class': class_names[true_idx],\n",
    "                    'Predicted_Class': class_names[pred_idx],\n",
    "                    'Count': count\n",
    "                })\n",
    "\n",
    "misclass_rf_df = pd.DataFrame(misclass_rf).sort_values('Count', ascending=False).head(10)\n",
    "print(\"Top-10 Misclassifications (RF Baseline):\")\n",
    "print(misclass_rf_df.to_string(index=False))\n",
    "misclass_rf_df.to_csv(os.path.join(OUTPUT_DIR, '06_1_rf_baseline_top10_errors.csv'), index=False)\n",
    "print(\"\\n Misclassification analysis salvata\\n\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"SECTION 6.1: RANDOM FOREST BASELINE – COMPLETED\")\n",
    "print(\"=\"*70 + \"\\n\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9c65c6c363b319b9",
   "metadata": {},
   "source": [
    "# SECTION 6.2: LIGHTGBM BASELINE – DETAILED ANALYSIS\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SECTION 6.2: LIGHTGBM BASELINE – DETAILED ANALYSIS\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# 1) Confusion Matrix\n",
    "cm_lgb = confusion_matrix(y_val_encoded, val_pred_lgb)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "sns.heatmap(\n",
    "    cm_lgb, annot=True, fmt='d', cmap=cmap_custom,\n",
    "    xticklabels=class_names, yticklabels=class_names,\n",
    "    cbar_kws={'label': 'Count'}, ax=ax,\n",
    "    linewidths=0.5, linecolor='gray',\n",
    "    annot_kws={'fontsize': 9}\n",
    ")\n",
    "ax.set_xlabel('Predicted Label', fontweight='bold', fontsize=11)\n",
    "ax.set_ylabel('True Label', fontweight='bold', fontsize=11)\n",
    "ax.set_title('LightGBM Baseline – Confusion Matrix (Validation Set)',\n",
    "             fontweight='bold', fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(IMG_DIR, '06_2_lgb_baseline_confusion_matrix.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\" Confusion matrix salvata\\n\")\n",
    "\n",
    "# 2) Per-class metrics\n",
    "report_lgb = classification_report(\n",
    "    y_val_encoded, val_pred_lgb,\n",
    "    target_names=class_names,\n",
    "    output_dict=True\n",
    ")\n",
    "perclass_lgb = pd.DataFrame(report_lgb).T.iloc[:-3, :]\n",
    "perclass_lgb['support'] = perclass_lgb['support'].astype(int)\n",
    "perclass_lgb.index.name = 'Class'\n",
    "\n",
    "print(\"Per-Class Metrics (LightGBM Baseline):\")\n",
    "print(perclass_lgb[['precision', 'recall', 'f1-score', 'support']].to_string())\n",
    "\n",
    "perclass_lgb.to_csv(os.path.join(OUTPUT_DIR, '06_2_lgb_baseline_perclass_metrics.csv'))\n",
    "print(\"\\n Per-class metrics salvate\\n\")\n",
    "\n",
    "# 3) Feature importance (Top-20)\n",
    "fi_lgb = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': lgb_baseline.feature_importances_\n",
    "}).sort_values('Importance', ascending=False).head(20)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "ax.barh(fi_lgb['Feature'], fi_lgb['Importance'], color='forestgreen', alpha=0.9)\n",
    "ax.invert_yaxis()\n",
    "ax.set_xlabel('Split Importance', fontweight='bold')\n",
    "ax.set_title('LightGBM Baseline – Top 20 Features by Importance', fontweight='bold')\n",
    "ax.grid(axis='x', linestyle='--', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(IMG_DIR, '06_2_lgb_baseline_feature_importance.png'), dpi=150)\n",
    "plt.show()\n",
    "print(\" Feature importance salvata\\n\")\n",
    "\n",
    "fi_lgb.to_csv(os.path.join(OUTPUT_DIR, '06_2_lgb_baseline_feature_importance.csv'), index=False)\n",
    "\n",
    "# 4) Misclassification analysis\n",
    "misclass_lgb = []\n",
    "for true_idx in range(len(class_names)):\n",
    "    for pred_idx in range(len(class_names)):\n",
    "        if true_idx != pred_idx:\n",
    "            count = cm_lgb[true_idx, pred_idx]\n",
    "            if count > 0:\n",
    "                misclass_lgb.append({\n",
    "                    'True_Class': class_names[true_idx],\n",
    "                    'Predicted_Class': class_names[pred_idx],\n",
    "                    'Count': count\n",
    "                })\n",
    "\n",
    "misclass_lgb_df = pd.DataFrame(misclass_lgb).sort_values('Count', ascending=False).head(10)\n",
    "print(\"Top-10 Misclassifications (LightGBM Baseline):\")\n",
    "print(misclass_lgb_df.to_string(index=False))\n",
    "misclass_lgb_df.to_csv(os.path.join(OUTPUT_DIR, '06_2_lgb_baseline_top10_errors.csv'), index=False)\n",
    "print(\"\\n Misclassification analysis salvata\\n\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"SECTION 6.2: LIGHTGBM BASELINE – COMPLETED\")\n",
    "print(\"=\"*70 + \"\\n\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "35b0f23fe7302323",
   "metadata": {},
   "source": [
    "# SECTION 6.3: MLP BASELINE – DETAILED ANALYSIS\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SECTION 6.3: MLP BASELINE – DETAILED ANALYSIS\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# 1) Confusion Matrix\n",
    "cm_mlp = confusion_matrix(y_val_encoded, val_pred_mlp)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "sns.heatmap(\n",
    "    cm_mlp, annot=True, fmt='d', cmap=cmap_custom,\n",
    "    xticklabels=class_names, yticklabels=class_names,\n",
    "    cbar_kws={'label': 'Count'}, ax=ax,\n",
    "    linewidths=0.5, linecolor='gray',\n",
    "    annot_kws={'fontsize': 9}\n",
    ")\n",
    "ax.set_xlabel('Predicted Label', fontweight='bold', fontsize=11)\n",
    "ax.set_ylabel('True Label', fontweight='bold', fontsize=11)\n",
    "ax.set_title('MLP Baseline – Confusion Matrix (Validation Set)',\n",
    "             fontweight='bold', fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(IMG_DIR, '06_3_mlp_baseline_confusion_matrix.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\" Confusion matrix salvata\\n\")\n",
    "\n",
    "# 2) Per-class metrics\n",
    "report_mlp = classification_report(\n",
    "    y_val_encoded, val_pred_mlp,\n",
    "    target_names=class_names,\n",
    "    output_dict=True\n",
    ")\n",
    "perclass_mlp = pd.DataFrame(report_mlp).T.iloc[:-3, :]\n",
    "perclass_mlp['support'] = perclass_mlp['support'].astype(int)\n",
    "perclass_mlp.index.name = 'Class'\n",
    "\n",
    "print(\"Per-Class Metrics (MLP Baseline):\")\n",
    "print(perclass_mlp[['precision', 'recall', 'f1-score', 'support']].to_string())\n",
    "\n",
    "perclass_mlp.to_csv(os.path.join(OUTPUT_DIR, '06_3_mlp_baseline_perclass_metrics.csv'))\n",
    "print(\"\\n Per-class metrics salvate\\n\")\n",
    "\n",
    "# 3) Training history plot (loss e accuracy)\n",
    "history_mlp_df = pd.DataFrame(history_mlp.history)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history_mlp_df.index, history_mlp_df['loss'], label='Train Loss', linewidth=2)\n",
    "axes[0].plot(history_mlp_df.index, history_mlp_df['val_loss'], label='Val Loss', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch', fontweight='bold')\n",
    "axes[0].set_ylabel('Loss', fontweight='bold')\n",
    "axes[0].set_title('MLP Baseline – Training Loss', fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[1].plot(history_mlp_df.index, history_mlp_df['accuracy'], label='Train Accuracy', linewidth=2)\n",
    "axes[1].plot(history_mlp_df.index, history_mlp_df['val_accuracy'], label='Val Accuracy', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch', fontweight='bold')\n",
    "axes[1].set_ylabel('Accuracy', fontweight='bold')\n",
    "axes[1].set_title('MLP Baseline – Training Accuracy', fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(IMG_DIR, '06_3_mlp_baseline_training_history.png'), dpi=150)\n",
    "plt.show()\n",
    "print(\" Training history salvata\\n\")\n",
    "\n",
    "history_mlp_df.to_csv(os.path.join(OUTPUT_DIR, '06_3_mlp_baseline_training_history.csv'), index_label='Epoch')\n",
    "\n",
    "# 4) Misclassification analysis\n",
    "misclass_mlp = []\n",
    "for true_idx in range(len(class_names)):\n",
    "    for pred_idx in range(len(class_names)):\n",
    "        if true_idx != pred_idx:\n",
    "            count = cm_mlp[true_idx, pred_idx]\n",
    "            if count > 0:\n",
    "                misclass_mlp.append({\n",
    "                    'True_Class': class_names[true_idx],\n",
    "                    'Predicted_Class': class_names[pred_idx],\n",
    "                    'Count': count\n",
    "                })\n",
    "\n",
    "misclass_mlp_df = pd.DataFrame(misclass_mlp).sort_values('Count', ascending=False).head(10)\n",
    "print(\"Top-10 Misclassifications (MLP Baseline):\")\n",
    "print(misclass_mlp_df.to_string(index=False))\n",
    "misclass_mlp_df.to_csv(os.path.join(OUTPUT_DIR, '06_3_mlp_baseline_top10_errors.csv'), index=False)\n",
    "print(\"\\n Misclassification analysis salvata\\n\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"SECTION 6.3: MLP BASELINE – COMPLETED\")\n",
    "print(\"=\"*70 + \"\\n\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cce8564577b3a4c",
   "metadata": {},
   "source": [
    "# SECTION 6.4: RANDOM FOREST + SMOTE – DETAILED ANALYSIS\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SECTION 6.4: RANDOM FOREST + SMOTE – DETAILED ANALYSIS\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# 1) Confusion Matrix\n",
    "cm_rf_smote = confusion_matrix(y_val_encoded, val_pred_rf_smote)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "sns.heatmap(\n",
    "    cm_rf_smote, annot=True, fmt='d', cmap=cmap_custom,\n",
    "    xticklabels=class_names, yticklabels=class_names,\n",
    "    cbar_kws={'label': 'Count'}, ax=ax,\n",
    "    linewidths=0.5, linecolor='gray',\n",
    "    annot_kws={'fontsize': 9}\n",
    ")\n",
    "ax.set_xlabel('Predicted Label', fontweight='bold', fontsize=11)\n",
    "ax.set_ylabel('True Label', fontweight='bold', fontsize=11)\n",
    "ax.set_title('Random Forest + SMOTE – Confusion Matrix (Validation Set)',\n",
    "             fontweight='bold', fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(IMG_DIR, '06_4_rf_smote_confusion_matrix.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\" Confusion matrix salvata\\n\")\n",
    "\n",
    "# 2) Per-class metrics\n",
    "report_rf_smote = classification_report(\n",
    "    y_val_encoded, val_pred_rf_smote,\n",
    "    target_names=class_names,\n",
    "    output_dict=True\n",
    ")\n",
    "perclass_rf_smote = pd.DataFrame(report_rf_smote).T.iloc[:-3, :]\n",
    "perclass_rf_smote['support'] = perclass_rf_smote['support'].astype(int)\n",
    "perclass_rf_smote.index.name = 'Class'\n",
    "\n",
    "print(\"Per-Class Metrics (RF + SMOTE):\")\n",
    "print(perclass_rf_smote[['precision', 'recall', 'f1-score', 'support']].to_string())\n",
    "\n",
    "perclass_rf_smote.to_csv(os.path.join(OUTPUT_DIR, '06_4_rf_smote_perclass_metrics.csv'))\n",
    "print(\"\\n Per-class metrics salvate\\n\")\n",
    "\n",
    "# 3) Confronto baseline vs SMOTE (RF)\n",
    "comparison_rf = pd.DataFrame({\n",
    "    'Class': class_names,\n",
    "    'F1_Baseline': [report_rf[c]['f1-score'] for c in class_names],\n",
    "    'F1_SMOTE': [report_rf_smote[c]['f1-score'] for c in class_names]\n",
    "})\n",
    "comparison_rf['Delta_F1'] = comparison_rf['F1_SMOTE'] - comparison_rf['F1_Baseline']\n",
    "\n",
    "print(\"Confronto F1-Score: RF Baseline vs RF+SMOTE:\")\n",
    "print(comparison_rf.to_string(index=False))\n",
    "comparison_rf.to_csv(os.path.join(OUTPUT_DIR, '06_4_rf_comparison_baseline_smote.csv'), index=False)\n",
    "print(\"\\n Confronto salvato\\n\")\n",
    "\n",
    "# Plot delta F1\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "colors = ['green' if x > 0 else 'red' for x in comparison_rf['Delta_F1']]\n",
    "ax.barh(comparison_rf['Class'], comparison_rf['Delta_F1'], color=colors, alpha=0.8)\n",
    "ax.axvline(0, color='black', linewidth=0.8, linestyle='--')\n",
    "ax.set_xlabel('ΔF1 (SMOTE - Baseline)', fontweight='bold')\n",
    "ax.set_title('Random Forest: Impact of SMOTE on F1-Score per Class', fontweight='bold')\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(IMG_DIR, '06_4_rf_smote_delta_f1.png'), dpi=150)\n",
    "plt.show()\n",
    "print(\" Delta F1 plot salvato\\n\")\n",
    "\n",
    "# 4) Feature importance\n",
    "fi_rf_smote = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': rf_smote.feature_importances_\n",
    "}).sort_values('Importance', ascending=False).head(20)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "ax.barh(fi_rf_smote['Feature'], fi_rf_smote['Importance'], color='steelblue', alpha=0.9)\n",
    "ax.invert_yaxis()\n",
    "ax.set_xlabel('Gini Importance', fontweight='bold')\n",
    "ax.set_title('Random Forest + SMOTE – Top 20 Features', fontweight='bold')\n",
    "ax.grid(axis='x', linestyle='--', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(IMG_DIR, '06_4_rf_smote_feature_importance.png'), dpi=150)\n",
    "plt.show()\n",
    "print(\" Feature importance salvata\\n\")\n",
    "\n",
    "fi_rf_smote.to_csv(os.path.join(OUTPUT_DIR, '06_4_rf_smote_feature_importance.csv'), index=False)\n",
    "\n",
    "# 5) Misclassification\n",
    "misclass_rf_smote = []\n",
    "for true_idx in range(len(class_names)):\n",
    "    for pred_idx in range(len(class_names)):\n",
    "        if true_idx != pred_idx:\n",
    "            count = cm_rf_smote[true_idx, pred_idx]\n",
    "            if count > 0:\n",
    "                misclass_rf_smote.append({\n",
    "                    'True_Class': class_names[true_idx],\n",
    "                    'Predicted_Class': class_names[pred_idx],\n",
    "                    'Count': count\n",
    "                })\n",
    "\n",
    "misclass_rf_smote_df = pd.DataFrame(misclass_rf_smote).sort_values('Count', ascending=False).head(10)\n",
    "print(\"Top-10 Misclassifications (RF + SMOTE):\")\n",
    "print(misclass_rf_smote_df.to_string(index=False))\n",
    "misclass_rf_smote_df.to_csv(os.path.join(OUTPUT_DIR, '06_4_rf_smote_top10_errors.csv'), index=False)\n",
    "print(\"\\n Misclassification analysis salvata\\n\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"SECTION 6.4: RANDOM FOREST + SMOTE – COMPLETED\")\n",
    "print(\"=\"*70 + \"\\n\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4becd01befbc97fa",
   "metadata": {},
   "source": [
    "# SECTION 6.5: LIGHTGBM + SMOTE – DETAILED ANALYSIS\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SECTION 6.5: LIGHTGBM + SMOTE – DETAILED ANALYSIS\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# 1) Confusion Matrix\n",
    "cm_lgb_smote = confusion_matrix(y_val_encoded, val_pred_lgb_smote)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "sns.heatmap(\n",
    "    cm_lgb_smote, annot=True, fmt='d', cmap=cmap_custom,\n",
    "    xticklabels=class_names, yticklabels=class_names,\n",
    "    cbar_kws={'label': 'Count'}, ax=ax,\n",
    "    linewidths=0.5, linecolor='gray',\n",
    "    annot_kws={'fontsize': 9}\n",
    ")\n",
    "ax.set_xlabel('Predicted Label', fontweight='bold', fontsize=11)\n",
    "ax.set_ylabel('True Label', fontweight='bold', fontsize=11)\n",
    "ax.set_title('LightGBM + SMOTE – Confusion Matrix (Validation Set)',\n",
    "             fontweight='bold', fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(IMG_DIR, '06_5_lgb_smote_confusion_matrix.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\" Confusion matrix salvata\\n\")\n",
    "\n",
    "# 2) Per-class metrics\n",
    "report_lgb_smote = classification_report(\n",
    "    y_val_encoded, val_pred_lgb_smote,\n",
    "    target_names=class_names,\n",
    "    output_dict=True\n",
    ")\n",
    "perclass_lgb_smote = pd.DataFrame(report_lgb_smote).T.iloc[:-3, :]\n",
    "perclass_lgb_smote['support'] = perclass_lgb_smote['support'].astype(int)\n",
    "perclass_lgb_smote.index.name = 'Class'\n",
    "\n",
    "print(\"Per-Class Metrics (LightGBM + SMOTE):\")\n",
    "print(perclass_lgb_smote[['precision', 'recall', 'f1-score', 'support']].to_string())\n",
    "\n",
    "perclass_lgb_smote.to_csv(os.path.join(OUTPUT_DIR, '06_5_lgb_smote_perclass_metrics.csv'))\n",
    "print(\"\\n Per-class metrics salvate\\n\")\n",
    "\n",
    "# 3) Confronto baseline vs SMOTE (LGB)\n",
    "comparison_lgb = pd.DataFrame({\n",
    "    'Class': class_names,\n",
    "    'F1_Baseline': [report_lgb[c]['f1-score'] for c in class_names],\n",
    "    'F1_SMOTE': [report_lgb_smote[c]['f1-score'] for c in class_names]\n",
    "})\n",
    "comparison_lgb['Delta_F1'] = comparison_lgb['F1_SMOTE'] - comparison_lgb['F1_Baseline']\n",
    "\n",
    "print(\"Confronto F1-Score: LGB Baseline vs LGB+SMOTE:\")\n",
    "print(comparison_lgb.to_string(index=False))\n",
    "comparison_lgb.to_csv(os.path.join(OUTPUT_DIR, '06_5_lgb_comparison_baseline_smote.csv'), index=False)\n",
    "print(\"\\n Confronto salvato\\n\")\n",
    "\n",
    "# Plot delta F1\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "colors = ['green' if x > 0 else 'red' for x in comparison_lgb['Delta_F1']]\n",
    "ax.barh(comparison_lgb['Class'], comparison_lgb['Delta_F1'], color=colors, alpha=0.8)\n",
    "ax.axvline(0, color='black', linewidth=0.8, linestyle='--')\n",
    "ax.set_xlabel('ΔF1 (SMOTE - Baseline)', fontweight='bold')\n",
    "ax.set_title('LightGBM: Impact of SMOTE on F1-Score per Class', fontweight='bold')\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(IMG_DIR, '06_5_lgb_smote_delta_f1.png'), dpi=150)\n",
    "plt.show()\n",
    "print(\" Delta F1 plot salvato\\n\")\n",
    "\n",
    "# 4) Feature importance\n",
    "fi_lgb_smote = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': lgb_smote.feature_importances_\n",
    "}).sort_values('Importance', ascending=False).head(20)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "ax.barh(fi_lgb_smote['Feature'], fi_lgb_smote['Importance'], color='forestgreen', alpha=0.9)\n",
    "ax.invert_yaxis()\n",
    "ax.set_xlabel('Split Importance', fontweight='bold')\n",
    "ax.set_title('LightGBM + SMOTE – Top 20 Features', fontweight='bold')\n",
    "ax.grid(axis='x', linestyle='--', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(IMG_DIR, '06_5_lgb_smote_feature_importance.png'), dpi=150)\n",
    "plt.show()\n",
    "print(\" Feature importance salvata\\n\")\n",
    "\n",
    "fi_lgb_smote.to_csv(os.path.join(OUTPUT_DIR, '06_5_lgb_smote_feature_importance.csv'), index=False)\n",
    "\n",
    "# 5) Misclassification\n",
    "misclass_lgb_smote = []\n",
    "for true_idx in range(len(class_names)):\n",
    "    for pred_idx in range(len(class_names)):\n",
    "        if true_idx != pred_idx:\n",
    "            count = cm_lgb_smote[true_idx, pred_idx]\n",
    "            if count > 0:\n",
    "                misclass_lgb_smote.append({\n",
    "                    'True_Class': class_names[true_idx],\n",
    "                    'Predicted_Class': class_names[pred_idx],\n",
    "                    'Count': count\n",
    "                })\n",
    "\n",
    "misclass_lgb_smote_df = pd.DataFrame(misclass_lgb_smote).sort_values('Count', ascending=False).head(10)\n",
    "print(\"Top-10 Misclassifications (LGB + SMOTE):\")\n",
    "print(misclass_lgb_smote_df.to_string(index=False))\n",
    "misclass_lgb_smote_df.to_csv(os.path.join(OUTPUT_DIR, '06_5_lgb_smote_top10_errors.csv'), index=False)\n",
    "print(\"\\n Misclassification analysis salvata\\n\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"SECTION 6.5: LIGHTGBM + SMOTE – COMPLETED\")\n",
    "print(\"=\"*70 + \"\\n\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "24af4e5f2542898b",
   "metadata": {},
   "source": [
    "# SECTION 6.6: MLP + SMOTE – DETAILED ANALYSIS\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SECTION 6.6: MLP + SMOTE – DETAILED ANALYSIS\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# 1) Confusion Matrix\n",
    "cm_mlp_smote = confusion_matrix(y_val_encoded, val_pred_mlp_smote)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "sns.heatmap(\n",
    "    cm_mlp_smote, annot=True, fmt='d', cmap=cmap_custom,\n",
    "    xticklabels=class_names, yticklabels=class_names,\n",
    "    cbar_kws={'label': 'Count'}, ax=ax,\n",
    "    linewidths=0.5, linecolor='gray',\n",
    "    annot_kws={'fontsize': 9}\n",
    ")\n",
    "ax.set_xlabel('Predicted Label', fontweight='bold', fontsize=11)\n",
    "ax.set_ylabel('True Label', fontweight='bold', fontsize=11)\n",
    "ax.set_title('MLP + SMOTE – Confusion Matrix (Validation Set)',\n",
    "             fontweight='bold', fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(IMG_DIR, '06_6_mlp_smote_confusion_matrix.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\" Confusion matrix salvata\\n\")\n",
    "\n",
    "# 2) Per-class metrics\n",
    "report_mlp_smote = classification_report(\n",
    "    y_val_encoded, val_pred_mlp_smote,\n",
    "    target_names=class_names,\n",
    "    output_dict=True\n",
    ")\n",
    "perclass_mlp_smote = pd.DataFrame(report_mlp_smote).T.iloc[:-3, :]\n",
    "perclass_mlp_smote['support'] = perclass_mlp_smote['support'].astype(int)\n",
    "perclass_mlp_smote.index.name = 'Class'\n",
    "\n",
    "print(\"Per-Class Metrics (MLP + SMOTE):\")\n",
    "print(perclass_mlp_smote[['precision', 'recall', 'f1-score', 'support']].to_string())\n",
    "\n",
    "perclass_mlp_smote.to_csv(os.path.join(OUTPUT_DIR, '06_6_mlp_smote_perclass_metrics.csv'))\n",
    "print(\"\\n Per-class metrics salvate\\n\")\n",
    "\n",
    "# 3) Confronto baseline vs SMOTE (MLP)\n",
    "comparison_mlp = pd.DataFrame({\n",
    "    'Class': class_names,\n",
    "    'F1_Baseline': [report_mlp[c]['f1-score'] for c in class_names],\n",
    "    'F1_SMOTE': [report_mlp_smote[c]['f1-score'] for c in class_names]\n",
    "})\n",
    "comparison_mlp['Delta_F1'] = comparison_mlp['F1_SMOTE'] - comparison_mlp['F1_Baseline']\n",
    "\n",
    "print(\"Confronto F1-Score: MLP Baseline vs MLP+SMOTE:\")\n",
    "print(comparison_mlp.to_string(index=False))\n",
    "comparison_mlp.to_csv(os.path.join(OUTPUT_DIR, '06_6_mlp_comparison_baseline_smote.csv'), index=False)\n",
    "print(\"\\n Confronto salvato\\n\")\n",
    "\n",
    "# Plot delta F1\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "colors = ['green' if x > 0 else 'red' for x in comparison_mlp['Delta_F1']]\n",
    "ax.barh(comparison_mlp['Class'], comparison_mlp['Delta_F1'], color=colors, alpha=0.8)\n",
    "ax.axvline(0, color='black', linewidth=0.8, linestyle='--')\n",
    "ax.set_xlabel('ΔF1 (SMOTE - Baseline)', fontweight='bold')\n",
    "ax.set_title('MLP: Impact of SMOTE on F1-Score per Class', fontweight='bold')\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(IMG_DIR, '06_6_mlp_smote_delta_f1.png'), dpi=150)\n",
    "plt.show()\n",
    "print(\" Delta F1 plot salvato\\n\")\n",
    "\n",
    "# 4) Training history\n",
    "history_mlp_smote_df = pd.DataFrame(history_mlp_smote.history)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history_mlp_smote_df.index, history_mlp_smote_df['loss'], label='Train Loss', linewidth=2)\n",
    "axes[0].plot(history_mlp_smote_df.index, history_mlp_smote_df['val_loss'], label='Val Loss', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch', fontweight='bold')\n",
    "axes[0].set_ylabel('Loss', fontweight='bold')\n",
    "axes[0].set_title('MLP + SMOTE – Training Loss', fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[1].plot(history_mlp_smote_df.index, history_mlp_smote_df['accuracy'], label='Train Accuracy', linewidth=2)\n",
    "axes[1].plot(history_mlp_smote_df.index, history_mlp_smote_df['val_accuracy'], label='Val Accuracy', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch', fontweight='bold')\n",
    "axes[1].set_ylabel('Accuracy', fontweight='bold')\n",
    "axes[1].set_title('MLP + SMOTE – Training Accuracy', fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(IMG_DIR, '06_6_mlp_smote_training_history.png'), dpi=150)\n",
    "plt.show()\n",
    "print(\" Training history salvata\\n\")\n",
    "\n",
    "history_mlp_smote_df.to_csv(os.path.join(OUTPUT_DIR, '06_6_mlp_smote_training_history.csv'), index_label='Epoch')\n",
    "\n",
    "# 5) Misclassification\n",
    "misclass_mlp_smote = []\n",
    "for true_idx in range(len(class_names)):\n",
    "    for pred_idx in range(len(class_names)):\n",
    "        if true_idx != pred_idx:\n",
    "            count = cm_mlp_smote[true_idx, pred_idx]\n",
    "            if count > 0:\n",
    "                misclass_mlp_smote.append({\n",
    "                    'True_Class': class_names[true_idx],\n",
    "                    'Predicted_Class': class_names[pred_idx],\n",
    "                    'Count': count\n",
    "                })\n",
    "\n",
    "misclass_mlp_smote_df = pd.DataFrame(misclass_mlp_smote).sort_values('Count', ascending=False).head(10)\n",
    "print(\"Top-10 Misclassifications (MLP + SMOTE):\")\n",
    "print(misclass_mlp_smote_df.to_string(index=False))\n",
    "misclass_mlp_smote_df.to_csv(os.path.join(OUTPUT_DIR, '06_6_mlp_smote_top10_errors.csv'), index=False)\n",
    "print(\"\\n Misclassification analysis salvata\\n\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"SECTION 6.6: MLP + SMOTE – COMPLETED\")\n",
    "print(\"=\"*70 + \"\\n\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a8ecbb39fc7d642d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# SECTION 7: Final Test Set Evaluation & Model Selection\n",
    "\n",
    "## Obiettivo\n",
    "\n",
    "1. **Selezionare il modello migliore** basandosi su validation performance\n",
    "2. **Valutare sul test set** (prima e unica volta)\n",
    "3. **Confrontare test vs validation** per verificare generalizzazione\n",
    "4. **Presentare risultati finali** con metriche complete\n",
    "\n",
    "## Workflow\n",
    "\n",
    "1. Selezione modello basata su validation F1-score macro\n",
    "2. Predizioni sul test set\n",
    "3. Confusion matrix test\n",
    "4. Per-class metrics test\n",
    "5. Confronto test/validation\n",
    "6. Dichiarazione performance finale\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "c7f70d38147c3c67",
   "metadata": {},
   "source": [
    "# SECTION 7.1: TEST SET PREDICTIONS\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SECTION 7: FINAL TEST SET EVALUATION\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "print(\"Generating predictions on test set (completely unseen data)...\\n\")\n",
    "\n",
    "# ========== BASELINE MODELS ==========\n",
    "print(\"[1/6] RF Baseline...\")\n",
    "test_pred_rf = rf_baseline.predict(X_test)\n",
    "acc_rf_test = accuracy_score(y_test_encoded, test_pred_rf)\n",
    "f1_rf_test = f1_score(y_test_encoded, test_pred_rf, average='macro')\n",
    "print(f\"   Test Accuracy: {acc_rf_test:.4f}, F1-Macro: {f1_rf_test:.4f}\")\n",
    "\n",
    "print(\"[2/6] LGB Baseline...\")\n",
    "test_pred_lgb = lgb_baseline.predict(X_test)\n",
    "acc_lgb_test = accuracy_score(y_test_encoded, test_pred_lgb)\n",
    "f1_lgb_test = f1_score(y_test_encoded, test_pred_lgb, average='macro')\n",
    "print(f\"   Test Accuracy: {acc_lgb_test:.4f}, F1-Macro: {f1_lgb_test:.4f}\")\n",
    "\n",
    "print(\"[3/6] MLP Baseline...\")\n",
    "test_pred_mlp_proba = mlp_baseline.predict(x_test_scaled, verbose=0)\n",
    "test_pred_mlp = np.argmax(test_pred_mlp_proba, axis=1)\n",
    "acc_mlp_test = accuracy_score(y_test_encoded, test_pred_mlp)\n",
    "f1_mlp_test = f1_score(y_test_encoded, test_pred_mlp, average='macro')\n",
    "print(f\"   Test Accuracy: {acc_mlp_test:.4f}, F1-Macro: {f1_mlp_test:.4f}\")\n",
    "\n",
    "# ========== SMOTE MODELS ==========\n",
    "print(\"[4/6] RF + SMOTE...\")\n",
    "test_pred_rf_smote = rf_smote.predict(X_test)\n",
    "acc_rf_smote_test = accuracy_score(y_test_encoded, test_pred_rf_smote)\n",
    "f1_rf_smote_test = f1_score(y_test_encoded, test_pred_rf_smote, average='macro')\n",
    "print(f\"   Test Accuracy: {acc_rf_smote_test:.4f}, F1-Macro: {f1_rf_smote_test:.4f}\")\n",
    "\n",
    "print(\"[5/6] LGB + SMOTE...\")\n",
    "test_pred_lgb_smote = lgb_smote.predict(X_test)\n",
    "acc_lgb_smote_test = accuracy_score(y_test_encoded, test_pred_lgb_smote)\n",
    "f1_lgb_smote_test = f1_score(y_test_encoded, test_pred_lgb_smote, average='macro')\n",
    "print(f\"   Test Accuracy: {acc_lgb_smote_test:.4f}, F1-Macro: {f1_lgb_smote_test:.4f}\")\n",
    "\n",
    "print(\"[6/6] MLP + SMOTE...\")\n",
    "test_pred_mlp_smote_proba = mlp_smote.predict(x_test_scaled, verbose=0)\n",
    "test_pred_mlp_smote = np.argmax(test_pred_mlp_smote_proba, axis=1)\n",
    "acc_mlp_smote_test = accuracy_score(y_test_encoded, test_pred_mlp_smote)\n",
    "f1_mlp_smote_test = f1_score(y_test_encoded, test_pred_mlp_smote, average='macro')\n",
    "print(f\"   Test Accuracy: {acc_mlp_smote_test:.4f}, F1-Macro: {f1_mlp_smote_test:.4f}\")\n",
    "\n",
    "print(\"\\n All test predictions generated\\n\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "dac631038c85084b",
   "metadata": {},
   "source": [
    "# SECTION 7.2: COMPREHENSIVE VALIDATION vs TEST COMPARISON\n",
    "\n",
    "results_final = pd.DataFrame({\n",
    "    'Model': [\n",
    "        'RF_Baseline', 'LGB_Baseline', 'MLP_Baseline',\n",
    "        'RF_SMOTE', 'LGB_SMOTE', 'MLP_SMOTE'\n",
    "    ],\n",
    "    'Val_Accuracy': [\n",
    "        acc_rf_val, acc_lgb_val, acc_mlp_val,\n",
    "        acc_rf_smote_val, acc_lgb_smote_val, acc_mlp_smote_val\n",
    "    ],\n",
    "    'Val_F1_Macro': [\n",
    "        f1_rf_val, f1_lgb_val, f1_mlp_val,\n",
    "        f1_rf_smote_val, f1_lgb_smote_val, f1_mlp_smote_val\n",
    "    ],\n",
    "    'Test_Accuracy': [\n",
    "        acc_rf_test, acc_lgb_test, acc_mlp_test,\n",
    "        acc_rf_smote_test, acc_lgb_smote_test, acc_mlp_smote_test\n",
    "    ],\n",
    "    'Test_F1_Macro': [\n",
    "        f1_rf_test, f1_lgb_test, f1_mlp_test,\n",
    "        f1_rf_smote_test, f1_lgb_smote_test, f1_mlp_smote_test\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Calcola gap (validation - test) per overfitting detection\n",
    "results_final['Gap_Accuracy'] = results_final['Val_Accuracy'] - results_final['Test_Accuracy']\n",
    "results_final['Gap_F1'] = results_final['Val_F1_Macro'] - results_final['Test_F1_Macro']\n",
    "\n",
    "# Ordina per Test F1-Macro (metrica principale)\n",
    "results_final = results_final.sort_values('Test_F1_Macro', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"COMPREHENSIVE RESULTS: VALIDATION vs TEST\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "print(results_final.to_string(index=False))\n",
    "\n",
    "results_final.to_csv(os.path.join(OUTPUT_DIR, '07_final_results_validation_vs_test.csv'), index=False)\n",
    "print(\"\\n Risultati salvati\\n\")\n",
    "\n",
    "# Identifica best model\n",
    "best_model_name = results_final.iloc[0]['Model']\n",
    "best_f1_test = results_final.iloc[0]['Test_F1_Macro']\n",
    "best_acc_test = results_final.iloc[0]['Test_Accuracy']\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"🏆 BEST MODEL IDENTIFIED\")\n",
    "print(\"=\"*70)\n",
    "print(f\"  Model:         {best_model_name}\")\n",
    "print(f\"  Test F1-Macro: {best_f1_test:.4f}\")\n",
    "print(f\"  Test Accuracy: {best_acc_test:.4f}\")\n",
    "print(\"=\"*70 + \"\\n\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "129de03fe80f0e3",
   "metadata": {},
   "source": [
    "# SECTION 7.3: VALIDATION vs TEST VISUALIZATION (Fixed xlim)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "colors_models = ['#2E86AB', '#A23B72', '#F18F01', '#2E86AB', '#A23B72', '#F18F01']\n",
    "\n",
    "# 1) Validation Accuracy\n",
    "bars1 = axes[0, 0].barh(results_final['Model'], results_final['Val_Accuracy'],\n",
    "                         color=colors_models, alpha=0.8, edgecolor='black')\n",
    "axes[0, 0].set_xlabel('Validation Accuracy', fontweight='bold', fontsize=11)\n",
    "axes[0, 0].set_title('Validation Accuracy by Model', fontweight='bold', fontsize=12)\n",
    "min_val_acc = results_final['Val_Accuracy'].min()\n",
    "axes[0, 0].set_xlim([max(0, min_val_acc - 0.05), 1.0])  #  max(0, ...) evita negativi\n",
    "axes[0, 0].grid(axis='x', alpha=0.3, linestyle='--')\n",
    "\n",
    "# 2) Test Accuracy\n",
    "bars2 = axes[0, 1].barh(results_final['Model'], results_final['Test_Accuracy'],\n",
    "                         color=colors_models, alpha=0.8, edgecolor='black')\n",
    "axes[0, 1].set_xlabel('Test Accuracy', fontweight='bold', fontsize=11)\n",
    "axes[0, 1].set_title('Test Accuracy by Model', fontweight='bold', fontsize=12)\n",
    "min_test_acc = results_final['Test_Accuracy'].min()\n",
    "axes[0, 1].set_xlim([max(0, min_test_acc - 0.05), 1.0])  # \n",
    "axes[0, 1].grid(axis='x', alpha=0.3, linestyle='--')\n",
    "\n",
    "# 3) Validation F1-Macro\n",
    "bars3 = axes[1, 0].barh(results_final['Model'], results_final['Val_F1_Macro'],\n",
    "                         color=colors_models, alpha=0.8, edgecolor='black')\n",
    "axes[1, 0].set_xlabel('Validation F1-Macro', fontweight='bold', fontsize=11)\n",
    "axes[1, 0].set_title('Validation F1-Macro by Model', fontweight='bold', fontsize=12)\n",
    "min_val_f1 = results_final['Val_F1_Macro'].min()\n",
    "axes[1, 0].set_xlim([max(0, min_val_f1 - 0.05), 1.0])  #  Ora mostra anche LGB_Baseline\n",
    "axes[1, 0].grid(axis='x', alpha=0.3, linestyle='--')\n",
    "\n",
    "# 4) Test F1-Macro\n",
    "bars4 = axes[1, 1].barh(results_final['Model'], results_final['Test_F1_Macro'],\n",
    "                         color=colors_models, alpha=0.8, edgecolor='black')\n",
    "axes[1, 1].set_xlabel('Test F1-Macro', fontweight='bold', fontsize=11)\n",
    "axes[1, 1].set_title('Test F1-Macro by Model (PRIMARY METRIC)', fontweight='bold', fontsize=12)\n",
    "min_test_f1 = results_final['Test_F1_Macro'].min()\n",
    "axes[1, 1].set_xlim([max(0, min_test_f1 - 0.05), 1.0])  # \n",
    "axes[1, 1].grid(axis='x', alpha=0.3, linestyle='--')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(IMG_DIR, '07_validation_vs_test_comparison.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\" Validation vs Test comparison plots salvati\\n\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b4fff02fe6000cf4",
   "metadata": {},
   "source": [
    "# SECTION 7.4: BEST MODEL - DETAILED TEST SET ANALYSIS\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"DETAILED ANALYSIS: {best_model_name} ON TEST SET\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Ottieni predictions del best model\n",
    "if best_model_name == 'RF_Baseline':\n",
    "    best_test_pred = test_pred_rf\n",
    "elif best_model_name == 'LGB_Baseline':\n",
    "    best_test_pred = test_pred_lgb\n",
    "elif best_model_name == 'MLP_Baseline':\n",
    "    best_test_pred = test_pred_mlp\n",
    "elif best_model_name == 'RF_SMOTE':\n",
    "    best_test_pred = test_pred_rf_smote\n",
    "elif best_model_name == 'LGB_SMOTE':\n",
    "    best_test_pred = test_pred_lgb_smote\n",
    "else:  # MLP_SMOTE\n",
    "    best_test_pred = test_pred_mlp_smote\n",
    "\n",
    "# Confusion Matrix\n",
    "cm_best = confusion_matrix(y_test_encoded, best_test_pred)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "sns.heatmap(\n",
    "    cm_best, annot=True, fmt='d', cmap=cmap_custom,\n",
    "    xticklabels=class_names, yticklabels=class_names,\n",
    "    cbar_kws={'label': 'Count'}, ax=ax,\n",
    "    linewidths=0.5, linecolor='gray',\n",
    "    annot_kws={'fontsize': 9}\n",
    ")\n",
    "ax.set_xlabel('Predicted Label', fontweight='bold', fontsize=11)\n",
    "ax.set_ylabel('True Label', fontweight='bold', fontsize=11)\n",
    "ax.set_title(f'{best_model_name} – Confusion Matrix (TEST SET)',\n",
    "             fontweight='bold', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(IMG_DIR, '07_best_model_test_confusion_matrix.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\" Best model confusion matrix salvata\\n\")\n",
    "\n",
    "# Per-class metrics\n",
    "report_best = classification_report(\n",
    "    y_test_encoded, best_test_pred,\n",
    "    target_names=class_names,\n",
    "    output_dict=True\n",
    ")\n",
    "perclass_best = pd.DataFrame(report_best).T.iloc[:-3, :]\n",
    "perclass_best['support'] = perclass_best['support'].astype(int)\n",
    "perclass_best.index.name = 'Class'\n",
    "\n",
    "print(f\"Per-Class Metrics ({best_model_name} on TEST SET):\")\n",
    "print(perclass_best[['precision', 'recall', 'f1-score', 'support']].to_string())\n",
    "\n",
    "perclass_best.to_csv(os.path.join(OUTPUT_DIR, '07_best_model_test_perclass_metrics.csv'))\n",
    "print(\"\\n Best model per-class metrics salvate\\n\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"SECTION 7 COMPLETED\")\n",
    "print(\"=\"*70 + \"\\n\")\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
